from __future__ import annotations

import argparse
import json
import re
import resource
import os
import subprocess
import traceback
import warnings
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime as dt
from pathlib import Path
from typing import Optional, Any, Union

import docker
import numpy as np
import tiktoken
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.utils._testing import ignore_warnings
from tqdm import tqdm

from editbench.config import SRC_INF_BENCHMARK_DATA, SRC_EXPERIMENTS
from editbench.config.constants import SRC_BENCHMARK_DATA
from editbench.evaluation.constants import KEY_INSTANCE_ID, KEY_MODEL, KEY_PREDICTION, RUN_EVALUATION_LOG_DIR, \
    INSTANCE_IMAGE_BUILD_DIR, LOG_INSTANCE, LOG_REPORT, DOCKER_PATCH, DOCKER_WORKDIR, DOCKER_USER, APPLY_PATCH_FAIL, \
    UTF8, APPLY_PATCH_PASS, LOG_TEST_OUTPUT, MAP_REPO_VERSION_TO_SPECS
from editbench.evaluation.docker_build import build_env_images, setup_logger, build_container, BuildImageError, \
    close_logger
from editbench.evaluation.docker_utils import list_images, copy_to_container, exec_run_with_timeout, should_remove, \
    cleanup_container, remove_image, clean_images, cleanup_excess_eval_images
from editbench.evaluation.grading import get_eval_report
from editbench.evaluation.test_spec import make_test_spec, TestSpec, apply_script
from editbench.evaluation.utils import load_inf_results, find_instance
from editbench.inference.run_api import MAP_MODEL_TO_COFIG, gpt_tokenize
from editbench.utils.dataset_utils import get_inf_datasets, normalize_dataset_name
from editbench.evaluation.metric_utils import preprocess_diff, python_code_tokenize, calculate_codebleu, jaccard_similarity, normalized_edit_distance, tfidf_cosine_similarity

class EvaluationError(Exception):
    def __init__(self, instance_id, message, logger):
        super().__init__(message)
        self.super_str = super().__str__()
        self.instance_id = instance_id
        self.log_file = logger.log_file
        self.logger = logger

    def __str__(self):
        return (
            f"Evaluation error for {self.instance_id}: {self.super_str}\n"
            f"Check ({self.log_file}) for more information."
        )


def get_model_name_path(prediction: dict) -> str:
    """
    get model path from prediction dictionary, support agent_name
    
    Args:
        prediction: prediction dictionary, may contain KEY_MODEL and agent_name fields
    
    Returns:
        model path string, if agent_name exists, return "agent_name/model_name", otherwise return "model_name"
    """
    model_name = prediction.get(KEY_MODEL, "None") or prediction.get("model_name_or_path", "None")
    agent_name = prediction.get("agent_name")
    if agent_name:
        return f"{agent_name}/{model_name.replace('/', '__')}"
    else:
        return model_name.replace("/", "__")


def make_run_report(
        predictions: dict,
        full_dataset: list,
        client: docker.DockerClient,
        run_id: str
) -> Path:
    """
    Make a final evaluation and run report of the instances that have been run.
    Also reports on images and containers that may still running!

    Args:
        predictions (dict): Predictions dict generated by the model
        full_dataset (list): List of all instances
        client (docker.DockerClient): Docker client
        run_id (str): Run ID

    Returns:
        Path to report file
    """
    # instantiate sets to store IDs of different outcomes
    completed_ids = set()
    resolved_ids = set()
    error_ids = set()
    unstopped_containers = set()
    unremoved_images = set()
    unresolved_ids = set()
    incomplete_ids = set()
    # get instances with empty patches
    empty_patch_ids = set()

    # iterate through dataset and check if the instance has been run
    for instance in full_dataset:
        instance_id = instance[KEY_INSTANCE_ID]
        if instance_id not in predictions:
            # skip instances without
            incomplete_ids.add(instance_id)
            continue
        prediction = predictions[instance_id]
        if prediction.get(KEY_PREDICTION, None) in ["", None]:
            empty_patch_ids.add(instance_id)
            continue
        report_file = (
                RUN_EVALUATION_LOG_DIR
                / run_id
                / get_model_name_path(prediction)
                / prediction[KEY_INSTANCE_ID]
                / LOG_REPORT
        )
        if report_file.exists():
            # If report file exists, then the instance has been run
            completed_ids.add(instance_id)
            report = json.loads(report_file.read_text())
            if report[instance_id]["resolved"]:
                # Record if the instance was resolved
                resolved_ids.add(instance_id)
            else:
                unresolved_ids.add(instance_id)
        else:
            # Otherwise, the instance was not run successfully
            error_ids.add(instance_id)

    # get remaining images and containers
    images = list_images(client)
    test_specs = list(map(make_test_spec, full_dataset))
    for spec in test_specs:
        image_name = spec.instance_image_key
        if image_name in images:
            unremoved_images.add(image_name)
    containers = client.containers.list(all=True)
    for container in containers:
        if run_id in container.name:
            unstopped_containers.add(container.name)

    # print final report
    dataset_ids = {i[KEY_INSTANCE_ID] for i in full_dataset}
    print(f"Total instances: {len(full_dataset)}")
    print(f"Instances submitted: {len(set(predictions.keys()) & dataset_ids)}")
    print(f"Instances completed: {len(completed_ids)}")
    print(f"Instances incomplete: {len(incomplete_ids)}")
    print(f"Instances resolved: {len(resolved_ids)}")
    print(f"Instances unresolved: {len(unresolved_ids)}")
    print(f"Instances with empty patches: {len(empty_patch_ids)}")
    print(f"Instances with errors: {len(error_ids)}")
    print(f"Unstopped containers: {len(unstopped_containers)}")
    print(f"Unremoved images: {len(unremoved_images)}")

    # write report to file
    report = {
        "total_instances": len(full_dataset),
        "submitted_instances": len(predictions),
        "completed_instances": len(completed_ids),
        "resolved_instances": len(resolved_ids),
        "unresolved_instances": len(unresolved_ids),
        "empty_patch_instances": len(empty_patch_ids),
        "error_instances": len(error_ids),
        "unstopped_instances": len(unstopped_containers),
        "completed_ids": list(sorted(completed_ids)),
        "incomplete_ids": list(sorted(incomplete_ids)),
        "empty_patch_ids": list(sorted(empty_patch_ids)),
        "submitted_ids": list(sorted(predictions.keys())),
        "resolved_ids": list(sorted(resolved_ids)),
        "unresolved_ids": list(sorted(unresolved_ids)),
        "error_ids": list(sorted(error_ids)),
        "unstopped_containers": list(sorted(unstopped_containers)),
        "unremoved_images": list(sorted(unremoved_images)),
        "schema_version": 2,
    }
    report_file = Path(
        get_model_name_path(list(predictions.values())[0])
        + f".{run_id}"
        + ".json"
    )
    with open(report_file, "w") as f:
        print(json.dumps(report, indent=4), file=f)
    print(f"Report written to {report_file}")
    return report_file


def run_instances(
        predictions: dict,
        instances: list,
        cache_level: str,
        clean: bool,
        force_rebuild: bool,
        max_workers: int,
        run_id: str,
        timeout: int,
        client: docker.DockerClient = None,
        max_eval_images: Optional[int] = None,
):
    """
    Run all instances for the given predictions in parallel.

    Args:
        predictions (dict): Predictions dict generated by the model
        instances (list): List of instances
        cache_level (str): Cache level
        clean (bool): Clean images above cache level
        force_rebuild (bool): Force rebuild images
        max_workers (int): Maximum number of workers
        run_id (str): Run ID
        timeout (int): Timeout for running tests
        client (docker.DockerClient): Docker client to reuse. If None, creates a new one.
        max_eval_images (int, optional): Maximum number of eval images to keep when cache_level="eval".
    """
    # reuse the passed Docker client, avoid creating new clients each time to accumulate file descriptors
    if client is None:
        client = docker.from_env()
    test_specs = list(map(make_test_spec, instances))

    # print number of existing instance images
    instance_image_ids = {x.instance_image_key for x in test_specs}
    # use the optimized list_images function, avoid "Too many open files" error
    try:
        all_images = list_images(client)
        print(f"Found {len(all_images)} existing images.")
        existing_images = {tag for tag in all_images if tag in instance_image_ids}
    except Exception as e:
        print(f"Warning: Failed to list images in run_instances: {e}")
        print("Continuing with empty image list...")
        existing_images = set()

    if not force_rebuild and len(existing_images):
        print(f"Found {len(existing_images)} existing instance images. Will reuse them.")

    # run instances in parallel
    print(f"Running {len(instances)} instances...")
    with tqdm(total=len(instances), smoothing=0) as pbar:
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(
                    run_instance,
                    test_spec,
                    predictions[test_spec.instance_id],
                    should_remove(
                        test_spec.instance_image_key,
                        cache_level,
                        clean,
                        set(),
                        client,
                        max_eval_images,
                    ),
                    force_rebuild,
                    client,
                    run_id,
                    timeout,
                ): None
                for test_spec in test_specs
            }
            # Wait for each future to complete
            for future in as_completed(futures):
                pbar.update(1)
                try:
                    # Update progress bar, check if instance ran successfully
                    future.result()
                except Exception as e:
                    traceback.print_exc()
                    continue
    print("All instances run.")


def run_instance(
        test_spec: TestSpec,
        pred: dict,
        rm_image: bool,
        force_rebuild: bool,
        client: docker.DockerClient,
        run_id: str,
        timeout=None,
):
    """
    Run a single instance with the given prediction.

    Args:
        test_spec (TestSpec): TestSpec instance
        pred (dict): Prediction w/ model_name_or_path, model_patch, instance_id
        rm_image (bool): Whether to remove the image after running
        force_rebuild (bool): Whether to force rebuild the image
        client (docker.DockerClient): Docker client
        run_id (str): Run ID
        timeout (int): Timeout for running tests
    """
    # Set up logging directory
    instance_id = test_spec.instance_id
    instance_id_rep = instance_id.replace("/", "__")
    model_name_or_path = get_model_name_path(pred)
    log_dir = RUN_EVALUATION_LOG_DIR / run_id / model_name_or_path / instance_id_rep
    log_dir.mkdir(parents=True, exist_ok=True)

    # Link the image build dir in the log dir
    build_dir = INSTANCE_IMAGE_BUILD_DIR / test_spec.instance_image_key.replace(":", "__")
    image_build_link = log_dir / "image_build_dir"
    if not image_build_link.exists():
        try:
            # link the image build dir in the log dir
            image_build_link.symlink_to(build_dir.absolute(), target_is_directory=True)
        except:
            # some error, idk why
            pass
    log_file = log_dir / LOG_INSTANCE

    # Set up report file + logger
    report_path = log_dir / LOG_REPORT
    if report_path.exists():
        msg = f"[{instance_id}] ‚è≠Ô∏è  skip: result already exists ({report_path})"
        print(msg)
        return instance_id, json.loads(report_path.read_text())
    logger = setup_logger(instance_id, log_file)

    # unified log output function
    def log_step(step_name: str, status: str = "in progress", detail: str = ""):
        """unified log output format"""
        status_icon = {
            "in progress": "üîÑ",
            "success": "‚úÖ",
            "failed": "‚ùå",
            "warning": "‚ö†Ô∏è",
            "information": "‚ÑπÔ∏è"
        }.get(status, "‚Ä¢")
        msg = f"[{instance_id}] {status_icon} {step_name}"
        if detail:
            msg += f": {detail}"
        print(msg)
        logger.info(f"[{status}] {step_name}" + (f": {detail}" if detail else ""))

    log_step("start instance execution", "in progress")
    # Run the instance
    container = None
    try:
        # report
        report = {}

        instance_id = pred[KEY_INSTANCE_ID]
        report[instance_id] = {
            "patch_is_None": False,
            "patch_exists": False,
            "patch_successfully_applied": False,
            "resolved": False,
        }

        # if not pred.get(KEY_PREDICTION).strip() or pred.get("status", "success") == "failed":
        if pred.get("status", "success") == "failed" or not pred.get(KEY_PREDICTION, "").strip():
            report[instance_id]["patch_is_None"] = True
            log_step("check patch", "failed", "patch is None or execution failed")
            raise EvaluationError(
                        instance_id,
                        f"{APPLY_PATCH_FAIL}: Patch is None",
                        logger,
                    )
        else:
            report[instance_id]["patch_exists"] = True
            log_step("check patch", "success", "patch exists")

        # Build + start instance container (instance image should already be built)
        log_step("build container", "in progress")
        
        container = build_container(test_spec, client, run_id, logger, False, force_rebuild)
        
        container.start()
        log_step("start container", "success", f"container ID: {container.id[:12]}")
        logger.info(f"Container for {instance_id} started: {container.id}")

        # Copy model prediction as patch file to container
        log_step("prepare patch file", "in progress")
        patch_file = Path(log_dir / "patch.diff")
        patch_file.write_text(pred[KEY_PREDICTION] + "\n" or "")
        logger.info(f"Intermediate patch for {instance_id} written to {patch_file}")

        # set patch apply bash
        patch_script_file = Path(log_dir / "apply.sh")
        apply_patch_script = apply_script("testbed", "/testbed",
                                          test_spec.instance.base_commit, pred.get("model_patch"),
                                          test_spec.instance.pre_edits)
        patch_script_file.write_text(apply_patch_script)
        copy_to_container(container, patch_script_file, Path("/apply.sh"))
        log_step("prepare patch file", "success", "copied to container")

        log_step("apply patch", "in progress", "trying to use apply.sh")
        val = container.exec_run(
            cmd="/bin/bash /apply.sh",
            workdir=DOCKER_WORKDIR,
            user=DOCKER_USER,
        )
        
        # record apply.sh output to log
        apply_output = val.output.decode(UTF8, errors='ignore')
        logger.info(f"=== apply.sh execution result (exit_code={val.exit_code}) ===")
        logger.info(f"apply.sh output:\n{apply_output}")
        logger.info(f"=== apply.sh execution result end ===")

        if val.exit_code != 0:
            copy_to_container(container, patch_file, Path(DOCKER_PATCH))
            log_step("apply patch", "warning", "apply.sh failed, trying git apply")
            logger.info(f"Failed to apply patch with apply.sh, trying git apply...")
            # Attempt to apply patch to container
            # note: git apply does not support --allow-empty option, remove it
            val = container.exec_run(
                f"git apply -v {DOCKER_PATCH}",
                workdir=DOCKER_WORKDIR,
                user=DOCKER_USER,
            )
            
            # record git apply output to log
            git_apply_output = val.output.decode(UTF8, errors='ignore')
            logger.info(f"=== git apply execution result (exit_code={val.exit_code}) ===")
            logger.info(f"git apply output:\n{git_apply_output}")
            logger.info(f"=== git apply execution result end ===")
            
            if val.exit_code != 0:
                log_step("apply patch", "warning", "git apply failed, trying patch command")
                # try "patch --batch --fuzz=5 -p1 -i {patch_path}" to try again
                val = container.exec_run(
                    f"patch --batch --fuzz=5 -p1 -i {DOCKER_PATCH}",
                    workdir=DOCKER_WORKDIR,
                    user=DOCKER_USER,
                )
                
                # record patch command output to log
                patch_output = val.output.decode(UTF8, errors='ignore')
                logger.info(f"=== patch command execution result (exit_code={val.exit_code}) ===")
                logger.info(f"patch command output:\n{patch_output}")
                logger.info(f"=== patch command execution result end ===")
                
                if val.exit_code != 0:
                    error_output = patch_output
                    log_step("apply patch", "failed", "all methods failed")
                    logger.info(f"{APPLY_PATCH_FAIL}:\n{error_output}")
                    report[instance_id]["patch_successfully_applied"] = False
                    raise EvaluationError(
                        instance_id,
                        f"{APPLY_PATCH_FAIL}:\n{error_output}",
                        logger,
                    )
                else:
                    log_step("apply patch", "success", "using patch command successfully")
                    logger.info(f"{APPLY_PATCH_PASS}:\n{patch_output}")
            else:
                log_step("apply patch", "success", "using git apply successfully")
                logger.info(f"{APPLY_PATCH_PASS}:\n{git_apply_output}")
        else:
            log_step("apply patch", "success", "using apply.sh successfully")
            logger.info(f"{APPLY_PATCH_PASS}:\n{apply_output}")

        report[instance_id]["patch_successfully_applied"] = True
        # Get git diff before running eval script
        log_step("check git status", "in progress")
        try:
            git_diff_output_before = (
                container.exec_run("git diff", workdir=DOCKER_WORKDIR).output.decode(UTF8, errors='replace').strip()
            )
            logger.info(f"Git diff before:\n{git_diff_output_before}")
        except UnicodeDecodeError as e:
            logger.warning(f"cannot fully decode Git diff output (UTF-8 error): {e}, using replace mode")
            git_diff_output_before = (
                container.exec_run("git diff", workdir=DOCKER_WORKDIR).output.decode(UTF8, errors='replace').strip()
            )
            logger.info(f"Git diff before (partially decoded):\n{git_diff_output_before[:1000]}...")
        except Exception as e:
            logger.warning(f"error getting Git diff: {e}, continue evaluation")
            git_diff_output_before = ""

        log_step("prepare eval script", "in progress")
        eval_file = Path(log_dir / "eval.sh")
        eval_file.write_text(test_spec.eval_script)
        copy_to_container(container, eval_file, Path("/eval.sh"))
        log_step("prepare eval script", "success", "copied to container")
        logger.info(f"Eval script for {instance_id} written to {eval_file}")

        log_step("run eval test", "in progress", f"timeout: {timeout} seconds")
        # Run eval script, write output to logs
        test_output, timed_out, total_runtime = exec_run_with_timeout(container,
                                                                      "/bin/bash /eval.sh", timeout)
        test_output_path = log_dir / LOG_TEST_OUTPUT
        logger.info(f'Test runtime: {total_runtime:_.2f} seconds')
        with open(test_output_path, "w") as f:
            f.write(test_output)
            logger.info(f"Test output for {instance_id} written to {test_output_path}")
            if timed_out:
                f.write(f"\n\nTimeout error: {timeout} seconds exceeded.")
                log_step("run eval test", "failed", f"timeout ({timeout} seconds)")
                raise EvaluationError(
                    instance_id,
                    f"Test timed out after {timeout} seconds.",
                    logger,
                )
        log_step("run eval test", "success", f"runtime: {total_runtime:.2f} seconds")
        # Get git diff after running eval script
        log_step("check git status", "in progress", "after running eval script")
        try:
            git_diff_output_after = (
                container.exec_run("git diff", workdir=DOCKER_WORKDIR).output.decode(UTF8, errors='replace').strip()
            )
            # Check if git diff changed after running eval script
            logger.info(f"Git diff after:\n{git_diff_output_after}")
            if git_diff_output_after != git_diff_output_before:
                log_step("check git status", "warning", "Git diff changed after running eval script")
                logger.info(f"Git diff changed after running eval script")
        except UnicodeDecodeError as e:
            logger.warning(f"cannot fully decode Git diff output (UTF-8 error): {e}, using replace mode")
            try:
                git_diff_output_after = (
                    container.exec_run("git diff", workdir=DOCKER_WORKDIR).output.decode(UTF8, errors='replace').strip()
                )
                logger.info(f"Git diff after (partially decoded):\n{git_diff_output_after[:1000]}...")
                if git_diff_output_after != git_diff_output_before:
                    log_step("check git status", "warning", "Git diff changed after running eval script (partially decoded)")
            except Exception as e2:
                logger.warning(f"error getting Git diff: {e2}, skip comparison")
        except Exception as e:
            logger.warning(f"error getting Git diff: {e}, continue evaluation")

        # Get report from test output
        log_step("generate eval report", "in progress")
        report = get_eval_report(
            test_spec=test_spec,
            prediction=pred,
            log_path=test_output_path,
            include_tests_status=True,
        )

        resolved = report[instance_id]['resolved']
        status_icon = "‚úÖ" if resolved else "‚ùå"
        log_step("generate eval report", "success", f"result: {'resolved' if resolved else 'unresolved'}")
        logger.info(f"Final report: {json.dumps(report, indent=2)}")
        print(f"[{instance_id}] {status_icon} final result: {'resolved (resolved)' if resolved else 'unresolved (unresolved)'}")
        
        # Write report to report.json
        with open(report_path, "w") as f:
            f.write(json.dumps(report, indent=4))
        log_step("save report", "success", f"saved to {report_path}")
        return instance_id, report
    except EvaluationError as e:
        error_msg = traceback.format_exc()
        logger.error(f"EvaluationError: {error_msg}")
        print(f"[{instance_id}] ‚ùå eval error: {str(e)}")
        logger.info(f"Final report: {json.dumps(report, indent=2)}")
        resolved = report.get(instance_id, {}).get('resolved', False)
        status_icon = "‚úÖ" if resolved else "‚ùå"
        print(f"[{instance_id}] {status_icon} final result: {'resolved' if resolved else 'unresolved'}")
        with open(report_path, "w") as f:
            f.write(json.dumps(report, indent=4))
        log_step("save report", "failed", f"saved to {report_path}")
    except BuildImageError as e:
        error_msg = traceback.format_exc()
        logger.error(f"BuildImageError: {error_msg}")
        print(f"[{instance_id}] ‚ùå build image error: {str(e)}")
    except Exception as e:
        error_msg = (f"Error in evaluating model for {instance_id}: {e}\n"
                     f"{traceback.format_exc()}\n"
                     f"Check ({logger.log_file}) for more information.")
        logger.error(error_msg)
        print(f"[{instance_id}] ‚ùå unexpected error: {str(e)}")
        print(f"[{instance_id}] ‚ÑπÔ∏è  please check log: {logger.log_file}")
    finally:
        print(f"[{instance_id}] üèÅ execution completed")
        # Remove instance container + image, close logger
        cleanup_container(client, container, logger)
        # delete testbed - using safer method, avoid opening too many file descriptors
        target_dir = build_dir / "testbed"
        if os.path.exists(target_dir):
            try:
                # using subprocess to call rm -rf, avoid Python opening too many file descriptors
                subprocess.run(['rm', '-rf', str(target_dir)], check=False, timeout=30)
                if logger:
                    logger.info(f"Successfully deleted directory: {target_dir}")
            except (OSError, subprocess.TimeoutExpired) as e:
                # if subprocess fails, fallback to shutil.rmtree, but limit depth
                try:
                    # first try to delete files, then delete directories
                    for root, dirs, files in os.walk(target_dir, topdown=False):
                        for name in files:
                            try:
                                os.remove(os.path.join(root, name))
                            except OSError:
                                pass
                        for name in dirs:
                            try:
                                os.rmdir(os.path.join(root, name))
                            except OSError:
                                pass
                    os.rmdir(target_dir)
                    if logger:
                        logger.info(f"Successfully deleted directory: {target_dir}")
                except OSError as e2:
                    if logger:
                        logger.warning(f"Failed to delete directory: {target_dir} - {e2}")
        if rm_image:
            log_step("remove image", "in progress", test_spec.instance_image_key)
            remove_image(client, test_spec.instance_image_key, logger)
            log_step("remove image", "success", test_spec.instance_image_key)
        else:
            log_step("keep image", "success", f"{test_spec.instance_image_key} (cache_level=eval)")
        if logger:
            close_logger(logger)
    return


def get_gold_predictions(dataset_name, split):
    inf_instances = get_inf_datasets(dataset_name, split)
    return [
        {
            KEY_INSTANCE_ID: instance.instance_id,
            KEY_PREDICTION: instance.ground_truth,
            KEY_MODEL: "gold"
        }
        for instance in inf_instances
    ]


def get_dataset_from_preds(
        dataset_name: str,
        split: Optional[str],
        instance_ids: list,
        predictions: dict,
        run_id: str,
        exclude_completed: bool = True,
):
    """
        Return only instances that have predictions and are in the dataset.
        If instance_ids is provided, only return instances with those IDs.
        If exclude_completed is True, only return instances that have not been run yet.
        If predictions contain IDs not in dataset, only return the maximum matching dataset (no error).
        If run results exist, only return instances that have been run (if exclude_completed=False).
        If sampled_ids_file is provided, only return instances that are in the sampled_instance_ids list from that file.
    """
    # count total number of predictions before filtering
    total_count = len(predictions)
    
    # count filtered tasks
    no_model_patch_field = []  # no model_patch field
    empty_model_patch = []  # model_patch field but empty
    failed_status = []  # status failed
    error_types = defaultdict(list)  # error type statistics
    
    filtered_predictions = {}
    for key, value in predictions.items():
        # check if there is model_patch field
        if KEY_PREDICTION not in value:
            no_model_patch_field.append(key)
            # count status and error (even if there is no model_patch field)
            status = value.get("status", "")
            if status == "failed":
                failed_status.append(key)
                error = value.get("error", "")
                if error:
                    if "Max retries" in error:
                        error_types["Max retries"].append(key)
                    else:
                        error_types[error[:50]].append(key)
            continue  # filter out: no model_patch field
        
        # keep valid predictions (model_patch is not empty)
        filtered_predictions[key] = value
    
    # output statistics
    print(f"\n{'='*80}")
    print(f"üìä prediction data filtering statistics:")
    print(f"{'='*80}")
    print(f"total tasks: {total_count}")
    print(f"valid tasks: {len(filtered_predictions)}")
    print(f"filtered tasks: {total_count - len(filtered_predictions)}")
    print(f"\nfilter reasons classification:")
    print(f"  - no {KEY_PREDICTION} field: {len(no_model_patch_field)}")
    
    print(f"  - status failed: {len(failed_status)}")
    if failed_status:
        print(f"    example: {failed_status[:5]}{'...' if len(failed_status) > 5 else ''}")
    
    if error_types:
        print(f"\nerror type statistics:")
        for error_type, ids in sorted(error_types.items(), key=lambda x: len(x[1]), reverse=True):
            print(f"  - {error_type}: {len(ids)}")
            if len(ids) <= 3:
                print(f"    task ID: {ids}")
            else:
                print(f"    example: {ids[:3]}{'...' if len(ids) > 3 else ''}")
    
    print(f"{'='*80}\n")
    
    predictions = filtered_predictions

    if instance_ids:
        # check that all instance IDs have predictions
        missing_preds = set(instance_ids) - set(predictions.keys())
        if missing_preds:
            print(f"Warning: Missing predictions for {len(missing_preds)} instance IDs.")
        predictions = {key: value for key, value in predictions.items() if key in instance_ids}

    # load dataset
    inf_datasets = get_inf_datasets(dataset_name, split)
    dataset_ids = {i.instance_id for i in inf_datasets}

    # if predictions has IDs not in dataset, only keep matching ones (no error)
    missing_ids = set(predictions.keys()) - dataset_ids
    if missing_ids:
        print(f"Warning: {len(missing_ids)} prediction IDs not found in dataset, will only return matching instances.")
        print(f"Missing IDs (first 10): {list(missing_ids)[:10]}")
        # only keep predictions in dataset
        predictions = {key: value for key, value in predictions.items() if key in dataset_ids}

    # filter out dataset instances in predictions
    inf_datasets = [i for i in inf_datasets if i.instance_id in predictions.keys()]
    # inf_datasets = [i for i in inf_datasets if "19214" in i.instance_id ]
    # check completed results
    completed_ids = set()
    if predictions:
        # get model name (from the first prediction)
        first_pred = next(iter(predictions.values()))
        model_name_or_path = get_model_name_path(first_pred)
        
        # check if each instance has been run
        for instance in inf_datasets:
            instance_id = instance.instance_id
            instance_id_rep = instance_id.replace("/", "__")
            report_file = (
                RUN_EVALUATION_LOG_DIR
                / run_id
                / model_name_or_path
                / instance_id_rep
                / LOG_REPORT
            )
            if report_file.exists():
                completed_ids.add(instance_id)

    # according to exclude_completed parameter, decide which instances to return
    if exclude_completed:
        # only return instances that have not been run
        inf_datasets = [i for i in inf_datasets if i.instance_id not in completed_ids]
        if completed_ids:
            print(f"Excluding {len(completed_ids)} already completed instances.")

    return inf_datasets


def main(
        dataset_name: str,
        split: str,
        instance_ids: list,
        predictions_path: str,
        max_workers: int,
        force_rebuild: bool,
        cache_level: str,
        clean: bool,
        open_file_limit: int,
        run_id: str,
        timeout: int,
        max_eval_images: Optional[int] = None,
):
    """
    Run evaluation harness for the given dataset and predictions.
    
    Args:
        dataset_name (str): Dataset name (can be "all", "owner/repo_name", or full path)
        split (str): Split name
        instance_ids (list): List of instance IDs
        predictions_path (str): Path to predictions file
        max_workers (int): Maximum number of workers
        force_rebuild (bool): Force rebuild images
        cache_level (str): Cache level ("none", "base", "env", "eval")
        clean (bool): Clean images above cache level
        open_file_limit (int): Maximum number of open files
        run_id (str): Run ID
        timeout (int): Timeout for running tests
        max_eval_images (int, optional): Maximum number of eval images to keep when cache_level="eval".
            If None, no limit. Only applies when cache_level="eval".
    """
    assert len(run_id) > 0, "Run ID must be provided"
    
    # Normalize dataset_name
    dataset_name, dataset_name_for_output = normalize_dataset_name(dataset_name, run_id)
    
    is_gold = predictions_path == "gold"
    # set open file limit - must be set before any file operations
    try:
        resource.setrlimit(resource.RLIMIT_NOFILE, (open_file_limit, open_file_limit))
        print(f"File descriptor limit set to: {open_file_limit}")
    except (ValueError, OSError) as e:
        current_limit = resource.getrlimit(resource.RLIMIT_NOFILE)
        print(f"Warning: Could not set file descriptor limit to {open_file_limit}. Current limit: {current_limit}")
        print(f"Error: {e}")

    client = docker.DockerClient()
    
    # validate max_eval_images parameter
    if cache_level == "eval" and max_eval_images is not None:
        if max_eval_images <= 0:
            print(f"Warning: max_eval_images={max_eval_images} is invalid, setting to None (no limit)")
            max_eval_images = None
        else:
            print(f"Cache level is 'eval', will keep at most {max_eval_images} eval images.")

    if is_gold:
        print("Using gold predictions - ignoring predictions_path")
        predictions = get_gold_predictions(dataset_name, split)
        # For gold, construct the output path for display
    else:
        if predictions_path.endswith(".json"):
            with open(predictions_path, "r") as f:
                predictions = json.load(f)
        elif predictions_path.endswith(".jsonl"):
            with open(predictions_path, "r") as f:
                predictions = [json.loads(line) for line in f]
        else:
            raise ValueError("Predictions path must be \"gold\", .json, or .jsonl")

    model_name = get_model_name_path(predictions[0])

    # 1. prediction results
    predictions = {pred[KEY_INSTANCE_ID]: pred for pred in predictions}

    # 2. datasets of prediction results: get datasets (for docker instance) from predictions
    dataset = get_dataset_from_preds(dataset_name, split, instance_ids,
                                     predictions, run_id, exclude_completed=True)

    print(f"Running {len(dataset)} unevaluated instances...")
    if not dataset:
        print("No instance to run.")
    else:
        predictions = {dataset_.instance_id: predictions[dataset_.instance_id] for dataset_ in dataset}
        build_env_images(client, dataset, force_rebuild, max_workers)
        run_instances(predictions, dataset, cache_level, clean, force_rebuild, max_workers, run_id, timeout, client, max_eval_images)

        # clean images + make final report
        clean_images(client, set(), cache_level, clean, max_eval_images)

    # get model configuration
    model_config = MAP_MODEL_TO_COFIG.get(model_name, {"temperature": 0, "n": 1})
    
    if is_gold:
        output = Path(f"{SRC_EXPERIMENTS}/gold/{dataset_name_for_output.replace('/', '-')}-{run_id}-output.json")
    else:
        output = Path(f"{SRC_EXPERIMENTS}/{model_name}/T={model_config['temperature']}/n={model_config['n']}/{dataset_name_for_output.replace('/', '-')}-{run_id}-output.json")
    print("\n" + "=" * 80)
    print(f"üöÄ evaluation results:")
    print(f"   - dataset: {dataset_name_for_output}")
    print(f"   - model: {model_name}")
    print(f"   - run ID: {run_id}")
    print(f"   - dataset input file: {dataset_name}")
    print(f"   - prediction file: {predictions_path}")
    print(f"   - output file: {output}")
    print("=" * 80 + "\n")
    # reuse existing client, avoid creating new connection causing file descriptor exhaustion
    # note: here cannot directly access sampled_instance_ids, need to pass instance_ids parameter when calling main
    # since instance_ids is already used in get_dataset_from_preds, analysis_results_from_report will use the same filtering
    # Pass "gold" if it was gold, otherwise pass the normalized path
    analysis_predictions_path = "gold" if is_gold else Path(predictions_path)
    analysis_results_from_report(dataset_name, run_id, client, model_name, output, analysis_predictions_path, target_instance_ids=instance_ids if instance_ids else None)


def analysis_results_from_report(
        inf_datasets: Any,
        run_id: str,
        client: docker.DockerClient,
        model_name: Optional[str] = None,
        output: Optional[Path] = None,
        prediction_path: Optional[Union[Path, str]] = None,
        target_instance_ids: Optional[list] = None
) -> dict:
    # before loading dataset, try to close some unused resources
    # force Python garbage collection, release file descriptors
    import gc
    gc.collect()
    
    try:
        # if target_instance_ids is specified, only load these instances
        if target_instance_ids:
            datasets = get_inf_datasets(inf_datasets, instance_ids=target_instance_ids)
            print(f"üìã use target instance_ids to filter: {len(target_instance_ids)} target IDs")
        else:
            datasets = get_inf_datasets(inf_datasets)
    except OSError as e:
        if e.errno == 24:  # too many open files
            print(f"Error: Too many open files when loading dataset. Error: {e}")
            print("Try increasing the file descriptor limit or closing other resources.")
            # try again to force garbage collection
            gc.collect()
            import time
            time.sleep(1)  # wait for file descriptors to be released
            # try again
            if target_instance_ids:
                datasets = get_inf_datasets(inf_datasets, instance_ids=target_instance_ids)
            else:
                datasets = get_inf_datasets(inf_datasets)
        else:
            raise
    
    if not datasets:
        raise ValueError("No valid datasets found after processing inf_datasets")

    report_dir = RUN_EVALUATION_LOG_DIR / run_id / model_name

    # if prediction_path is provided, load all predictions
    predictions_dict = {}
    prediction_file_path = None
    if prediction_path:
        if prediction_path == "gold":
            # For gold predictions, get ground truth from datasets
            print(f"\nüìã using gold predictions from dataset")
            for instance in datasets:
                predictions_dict[instance.instance_id] = {
                    'instance_id': instance.instance_id,
                    'model_patch': instance.ground_truth if hasattr(instance, 'ground_truth') else ""
                }
            print(f"loaded {len(predictions_dict)} gold predictions from dataset")
        else:
            prediction_file_path = Path(prediction_path)
            if not prediction_file_path.exists():
                raise FileNotFoundError(f"prediction file not found: {prediction_file_path}")
            
            print(f"\nüìã load predictions from prediction file: {prediction_file_path}")
            with open(prediction_file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        data = json.loads(line.strip())
                        instance_id = data.get('instance_id')
                        if instance_id:
                            predictions_dict[instance_id] = data
                    except json.JSONDecodeError:
                        continue
            print(f"loaded {len(predictions_dict)} predictions")

    # first step: collect all preprocessed patches, for TF-IDF fitting
    print(f"\nüìã collect patch data ({len(datasets)} instances)...")
    all_processed_patches = []
    for instance in tqdm(datasets, desc="collect patches", unit="instance"):
        # collect ground truth patch
        ground_truth_diff = instance.ground_truth.strip() if hasattr(instance, "ground_truth") else ""
        processed_gt = preprocess_diff(ground_truth_diff)
        all_processed_patches.append(processed_gt)

        # collect prediction patch
        if prediction_file_path and instance.instance_id in predictions_dict:
            # get model_patch from prediction results
            pred_data = predictions_dict[instance.instance_id]
            pred_diff = pred_data.get('model_patch', '').strip()
            processed_pred = preprocess_diff(pred_diff)
            all_processed_patches.append(processed_pred)
        else:
            # read prediction patch from file
            prediction_patch_path = report_dir / instance.instance_id / "patch.diff"
            if prediction_patch_path.exists():
                pred_diff = prediction_patch_path.read_text().strip()
                processed_pred = preprocess_diff(pred_diff)
                all_processed_patches.append(processed_pred)

    # initialize TF-IDF vectorizer (python code tokenizer)
    # suppress sklearn warning about token_pattern (when using custom tokenizer, token_pattern will be ignored)
    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", message=".*token_pattern.*", category=UserWarning)
        vectorizer = TfidfVectorizer(
            tokenizer=python_code_tokenize,
            token_pattern=None,  # explicitly set to None, avoid warning
            lowercase=False,  # code is case-sensitive (e.g. Variable and variable are different)
            stop_words=None  # code has no traditional stop words, keep all tokens
        )
    if any(all_processed_patches):  # only fit if there are valid patches
        vectorizer.fit(all_processed_patches)

    # initialize sets needed for basic report
    completed_ids = set()
    resolved_ids = set()
    applied_ids = set()
    unstopped_containers = set()
    unremoved_images = set()
    unresolved_ids = set()
    incomplete_ids = set()
    empty_patch_ids = set()

    # initialize result dictionary
    result = {
        "%applied": 0.0,
        "%resolved": 0.0,
        "avg_CodeBLEU": 0.0,
        "avg_normalized_edit_distance": 0.0,
        "avg_jaccard_similarity": 0.0,
        "avg_tfidf_cosine_similarity": 0.0,
        "avg_file_hit_rate": 0.0,  # average file hit rate (recall)
        "instance_details": []  # single instance detailed metrics
    }

    # iterate over instances to calculate all metrics
    metrics_accumulator = {
        "codebleu": [],
        "edit_distance": [],
        "jaccard": [],
        "tfidf_cosine": [],
        "file_hit_rate": []  # file hit rate list
    }
    
    # regex for extracting file names
    file_pattern = re.compile(r'diff --git a/(\S+) b/\S+')

    print(f"\nüìä start evaluating {len(datasets)} task instances...")
    for instance in tqdm(datasets, desc="evaluating progress", unit="instance"):
        instance_id = instance.instance_id
        report_path = report_dir / instance_id / "report.json"

        # read prediction patch: prefer from prediction_file_path, otherwise from file
        raw_pred_diff = None
        if prediction_path == "gold" or (prediction_file_path and instance_id in predictions_dict):
            # get model_patch from prediction results
            if prediction_path == "gold":
                # For gold, use ground_truth from dataset
                raw_pred_diff = instance.ground_truth.strip() if hasattr(instance, "ground_truth") else ""
            else:
                pred_data = predictions_dict[instance_id]
                raw_pred_diff = pred_data.get('model_patch', '').strip()
        else:
            incomplete_ids.add(instance_id)
            continue


        # skip instances with no report (if prediction_file_path is provided, report.json may not be needed)
        if not report_path.exists():
            incomplete_ids.add(instance_id)
            continue
        else:
            # use context manager to ensure file is correctly closed
            try:
                with open(report_path, "r", encoding="utf-8") as f:
                    report = json.load(f)[instance_id]
            except Exception as e:
                incomplete_ids.add(instance_id)
                continue

        completed_ids.add(instance_id)
        
        raw_gt_diff = instance.ground_truth.strip() if hasattr(instance, "ground_truth") else ""

        if report.get("patch_is_None", ""):
            empty_patch_ids.add(instance_id)
            continue

        # calculate applied and resolved (keep original logic)
        if report["resolved"]:
            applied_ids.add(instance_id)
            resolved_ids.add(instance_id)
        elif report["patch_successfully_applied"]:
            applied_ids.add(instance_id)
            unresolved_ids.add(instance_id)
        else:
            unresolved_ids.add(instance_id)

        # ------------------------------
        # core: preprocess diff, extract valid code
        # ------------------------------
        processed_gt = preprocess_diff(raw_gt_diff)
        processed_pred = preprocess_diff(raw_pred_diff)

        # ------------------------------
        # calculate all similarity metrics
        # ------------------------------
        instance_metrics = {
            "instance_id": instance_id,
            "raw_ground_truth": raw_gt_diff[:200] + "..." if len(raw_gt_diff) > 200 else raw_gt_diff,
            "raw_prediction": raw_pred_diff[:200] + "..." if len(raw_pred_diff) > 200 else raw_pred_diff,
            "processed_ground_truth": processed_gt[:200] + "..." if len(processed_gt) > 200 else processed_gt,
            "processed_prediction": processed_pred[:200] + "..." if len(processed_pred) > 200 else processed_pred,
            "resolved": report["resolved"],
            "applied": report["patch_successfully_applied"]
        }

        # 1. CodeBLEU (Python code-specific)
        codebleu = calculate_codebleu(processed_gt, processed_pred)
        metrics_accumulator["codebleu"].append(codebleu)
        instance_metrics["CodeBLEU"] = codebleu

        # 2. normalized edit distance
        edit_dist = normalized_edit_distance(processed_gt, processed_pred)
        metrics_accumulator["edit_distance"].append(edit_dist)
        instance_metrics["normalized_edit_distance"] = edit_dist

        # 3. Jaccard similarity (based on Python tokenization)
        gt_tokens = python_code_tokenize(processed_gt)
        pred_tokens = python_code_tokenize(processed_pred)
        jaccard_sim = jaccard_similarity(gt_tokens, pred_tokens)
        metrics_accumulator["jaccard"].append(jaccard_sim)
        instance_metrics["jaccard_similarity"] = jaccard_sim

        # 4. TF-IDF+cosine similarity
        if any(all_processed_patches):  # ensure vectorizer is fitted
            tfidf_sim = tfidf_cosine_similarity(processed_gt, processed_pred, vectorizer)
        else:
            tfidf_sim = 0.0
        metrics_accumulator["tfidf_cosine"].append(tfidf_sim)
        instance_metrics["tfidf_cosine_similarity"] = tfidf_sim

        # 5. file hit rate (recall)
        gt_files = set(file_pattern.findall(raw_gt_diff))
        model_files = set(file_pattern.findall(raw_pred_diff))
        hit_files = model_files & gt_files
        if len(gt_files) > 0:
            file_hit_rate = len(hit_files) / len(gt_files)
        else:
            file_hit_rate = 0.0  # if ground truth has no files, hit rate is 0
        metrics_accumulator["file_hit_rate"].append(file_hit_rate)
        instance_metrics["file_hit_rate"] = file_hit_rate
        instance_metrics["gt_files_count"] = len(gt_files)
        instance_metrics["model_files_count"] = len(model_files)
        instance_metrics["hit_files_count"] = len(hit_files)
        instance_metrics["gt_files"] = list(gt_files)
        instance_metrics["model_files"] = list(model_files)
        instance_metrics["hit_files"] = list(hit_files)

        # store single instance details
        result["instance_details"].append(instance_metrics)


    print("\nüîç scanning images and containers in Docker daemon...")
    # get remaining images and containers
    images = list_images(client)
    test_specs = list(map(make_test_spec, datasets))
    for spec in test_specs:
        image_name = spec.instance_image_key
        if image_name in images:
            unremoved_images.add(image_name)
    containers = client.containers.list(all=True)
    for container in containers:
        if run_id in container.name:
            unstopped_containers.add(container.name)

    print("\nüî¢ start calculating results...")  # added icon
    # ------------------------------
    # calculate average (avoid division by 0)
    # ------------------------------
    result.update(
        {
        "total_instances": len(datasets),
        "completed_instances": len(completed_ids),
        "incompleted_instances": len(incomplete_ids),
        "resolved_instances": len(resolved_ids),
        "applied_instances": len(applied_ids),
        "unresolved_instances": len(unresolved_ids),
        "empty_patch_instances": len(empty_patch_ids),
        "unstopped_instances": len(unstopped_containers),
        "completed_ids": list(sorted(completed_ids)),
        "incomplete_ids": list(sorted(incomplete_ids)),
        "empty_patch_ids": list(sorted(empty_patch_ids)),
        "resolved_ids": list(sorted(resolved_ids)),
        "unresolved_ids": list(sorted(unresolved_ids)),
        "unstopped_containers": list(sorted(unstopped_containers)),
        "unremoved_images": list(sorted(unremoved_images)),
        "schema_version": 1,
    })

    total = result["completed_instances"]
    if total > 0:
        # applied and resolved rates
        result["%applied"] = round((result["applied_instances"] / total) * 100, 2)
        result["%resolved"] = round((result["resolved_instances"] / total) * 100, 2)
        # average metrics (use np.mean more stable, automatically ignore outliers)
        result["avg_CodeBLEU"] = round(np.mean(metrics_accumulator["codebleu"]), 4)
        result["avg_normalized_edit_distance"] = round(np.mean(metrics_accumulator["edit_distance"]), 4)
        result["avg_jaccard_similarity"] = round(np.mean(metrics_accumulator["jaccard"]), 4)
        result["avg_tfidf_cosine_similarity"] = round(np.mean(metrics_accumulator["tfidf_cosine"]), 4)
        result["avg_file_hit_rate"] = round(np.mean(metrics_accumulator["file_hit_rate"]), 4)

    # save detailed results to file
    if output:
        # Ensure output directory exists
        output.parent.mkdir(parents=True, exist_ok=True)
        with open(output, "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
            # ------------------------------
            # print all key information
            # ------------------------------
    print("\n" + "=" * 80)
    print(f"[complete analysis report] Run ID: {run_id} | model name: {model_name}")
    print("=" * 80)

    # 1. basic instance statistics
    print("\n„Äêbasic instance statistics„Äë")
    print(f"total instances: {result['total_instances']}\n")
    print(f"completed instances: {result['completed_instances']}")
    print(f"incompleted instances: {result['incompleted_instances']}")
    print(f"resolved instances: {result['resolved_instances']}")
    print(f"applied instances: {result['applied_instances']}")
    print(f"unresolved instances: {result['unresolved_instances']}")
    print(f"empty patch instances: {result['empty_patch_instances']}")

    # 2. applied and resolved rate statistics
    print("\n„Äêapplied and resolved rate statistics„Äë\n")
    print(f"applied rate: {result['%applied']}%")
    print(f"resolved rate: {result['%resolved']}%")

    # 3. similarity metrics average values
    print("\n„Äêsimilarity metrics average values„Äë")
    print(f"avg CodeBLEU: {result['avg_CodeBLEU']}")
    print(f"avg normalized edit distance: {result['avg_normalized_edit_distance']}")
    print(f"avg Jaccard similarity: {result['avg_jaccard_similarity']}")
    print(f"avg TF-IDF cosine similarity: {result['avg_tfidf_cosine_similarity']}")
    print(f"avg file hit rate (recall): {result['avg_file_hit_rate']}")

    # 4. Docker resources statistics
    # print("\n„ÄêDocker resources statistics„Äë")
    # print(f"unstopped containers count: {len(unstopped_containers)}")
    # if unstopped_containers:
    #     print(f"unstopped containers list: {', '.join(unstopped_containers)}")
    # print(f"unremoved images count: {len(unremoved_images)}")
    # if unremoved_images:
    #     print(f"unremoved images list: {', '.join(unremoved_images)}")

    # 5. key ID list (optional display, default only display count, if need to display full list, uncomment)
    print("\n„Äêkey ID statistics„Äë")
    print(f"completed instances ID count: {len(result['completed_ids'])}")
    print(f"incompleted instances ID count: {len(result['incomplete_ids'])}")
    print(f"resolved instances ID count: {len(result['resolved_ids'])}")
    print(f"unresolved instances ID count: {len(result['unresolved_ids'])}")
    print(f"empty patch instances ID count: {len(result['empty_patch_ids'])}")

    print("\n" + "=" * 80)

    return result


def check_patch(predication_path: Union[Path, str], gt_path: Union[Path, str], evaluation_path: Union[Path, str]):

    if not Path(predication_path).exists() or not Path(gt_path).exists() or not Path(evaluation_path).exists():
        raise FileNotFoundError

    inf_data = get_inf_datasets(dataset_name)

    inf_data = {inf.instance_id: inf for inf in inf_data}

    evaluation_res = json.loads(Path(evaluation_path).read_text()).get("instance_details")

    evaluation_res = {res.get("instance_id"): res for res in evaluation_res}

    recall_all = 0
    resolved_all = 0
    applied_all = 0
    cnt = 0
    token_avg = 0
    with open(predication_path, mode="r", encoding="utf-8") as fr:
        for line in fr:
            data = json.loads(line)
            instance_id = data.get("instance_id")
            print(f"\n============„Äêtask id:„Äë=============\n{instance_id}")
            inf = inf_data[instance_id]
            encoding = tiktoken.encoding_for_model("gpt-5-codex")
            token_avg += gpt_tokenize(inf.prompt, encoding=encoding)
            cnt += 1
            res = evaluation_res[instance_id]
            resolved = res.get("resolved")
            if resolved:
                resolved_all += 1
            print(f"============„Äêwhether resolved:„Äë=============\n{resolved}")
            applied = res.get("applied")
            if applied:
                applied_all += 1
            print(f"============„Äêwhether applied:„Äë=============\n{applied}")
            blue = res.get("CodeBLEU")
            print(f"============„ÄêCodeBLEU:„Äë=============\n{blue}")
            ed = res.get("normalized_edit_distance")
            print(f"============„ÄêED:„Äë=============\n{ed}")
            len_pre_edits = len(inf.pre_edits)
            pre_edits = "\n".join(inf.pre_edits)
            print(f"============„Äêprevious edits, {len_pre_edits} steps:„Äë=============\n{pre_edits}")
            pattern = r'diff --git a/(\S+) b/\S+'
            gt_patch = inf.ground_truth
            gt_files = set(re.findall(pattern, gt_patch))
            print(f"\n============„Äêground truth content:„Äë=============\n{gt_patch}")
            model_patch = data.get("model_patch")
            model_files = set(re.findall(pattern, model_patch))
            print(f"\n============„Äêmodel prediction content:„Äë=============\n{model_patch}")
            hit_files = model_files & gt_files
            recall = len(hit_files) / len(gt_files)
            recall_all += recall
            print(f"\n============„Äêfile hit rate:„Äë=============\n{recall}")
            # if recall < 1.0:
            #     continue
            # if detect_import_edits(gt_patch)[0]:
            #     continue
            # if not resolved:
            #     continue
            # if len_pre_edits >= 2:
            #     continue
    print(f"total resolved_rateÔºö{resolved_all / cnt}")
    print(f"total applied_rateÔºö{applied_all / cnt}")
    print(f"total file_rateÔºö{recall_all / cnt}")
    print(f"avg_tokenÔºö{token_avg / cnt}")


def read_analysis_report(report_path: Union[Path, str]) -> dict:
    """
    convenient read complete analysis report, extract key metrics
    
    Args:
        report_path: analysis report JSON file path
    
    Returns:
        dictionary containing key metrics:
        - applied_rate: applied rate (%)
        - resolved_rate: resolved rate (%)
        - file_hit_rate: file hit rate (recall)
        - avg_CodeBLEU: avg CodeBLEU
        - avg_normalized_edit_distance: avg normalized edit distance
        - avg_jaccard_similarity: avg Jaccard similarity
        - avg_tfidf_cosine_similarity: avg TF-IDF cosine similarity
        - total_instances: total instances
        - completed_instances: completed instances
        - resolved_instances: resolved instances
        - applied_instances: applied instances
    """
    report_path = Path(report_path)
    if not report_path.exists():
        raise FileNotFoundError(f"report file not found: {report_path}")
    
    with open(report_path, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    # extract key metrics
    key_metrics = {
        "applied_rate": report.get("%applied", 0.0),
        "resolved_rate": report.get("%resolved", 0.0),
        "file_hit_rate": report.get("avg_file_hit_rate", 0.0),
        "avg_CodeBLEU": report.get("avg_CodeBLEU", 0.0),
        "avg_normalized_edit_distance": report.get("avg_normalized_edit_distance", 0.0),
        "avg_jaccard_similarity": report.get("avg_jaccard_similarity", 0.0),
        "avg_tfidf_cosine_similarity": report.get("avg_tfidf_cosine_similarity", 0.0),
        "total_instances": report.get("total_instances", 0),
        "completed_instances": report.get("completed_instances", 0),
        "resolved_instances": report.get("resolved_instances", 0),
        "applied_instances": report.get("applied_instances", 0),
    }
    
    return key_metrics


def print_analysis_summary_filtered(
    report_path: Union[Path, str],
    filter_ids_file: Union[Path, str],
    cutoff_date: Optional[str] = None,
    date_mode: str = "before",
):
    """
    print performance metrics after removing specified ids from existing output (without saving)
    
    Args:
        report_path: analysis report JSON file path
        filter_ids_file: JSON file path containing instance_ids to filter
        cutoff_date: cutoff date (optional), format as YYYYMMDD or YYYY-MM-DD
        date_mode: with cutoff_date. "before" only count instances before the date; "after" only count instances after the date
    """
    import numpy as np
    
    report_path = Path(report_path)
    filter_ids_file = Path(filter_ids_file)
    
    if not report_path.exists():
        raise FileNotFoundError(f"report file not found: {report_path}")
    if not filter_ids_file.exists():
        raise FileNotFoundError(f"filter IDs file not found: {filter_ids_file}")
    
    # read filter IDs
    with open(filter_ids_file, 'r', encoding='utf-8') as f:
        filter_data = json.load(f)
        filter_ids = set(filter_data.get('union_filter_instance_ids', []))
    
    print(f"read {len(filter_ids)} instance IDs to filter")
    
    # read original report
    with open(report_path, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    # filter instance_details (first by filter_ids)
    original_count = len(report.get('instance_details', []))
    details = [
        detail for detail in report.get('instance_details', [])
        if detail.get('instance_id') not in filter_ids
    ]
    
    # filter by date (if cutoff_date is provided)
    run_id = None
    filename = report_path.stem
    if "-output" in filename:
        match = re.search(r'(.+?)-output$', filename)
        if match:
            prefix = match.group(1)
            last_dash_idx = prefix.rfind('-')
            if last_dash_idx != -1:
                run_id = prefix[last_dash_idx + 1:]
            else:
                run_id = prefix
    
    if cutoff_date is not None and run_id:
        cutoff = _parse_cutoff_date(cutoff_date)
        dataset_file = SRC_INF_BENCHMARK_DATA / f"all-task-instances_{run_id}.jsonl"
        if dataset_file.exists():
            instance_date_map = {}
            with open(dataset_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    try:
                        data = json.loads(line)
                        iid = data.get("instance_id")
                        ca = data.get("created_at", "")
                        if iid:
                            instance_date_map[iid] = _parse_created_at(ca)
                    except json.JSONDecodeError:
                        continue
            if date_mode == "before":
                filtered_details = [
                    d for d in details
                    if instance_date_map.get(d.get("instance_id")) is not None
                    and instance_date_map[d["instance_id"]] < cutoff
                ]
            else:
                filtered_details = [
                    d for d in details
                    if instance_date_map.get(d.get("instance_id")) is not None
                    and instance_date_map[d["instance_id"]] >= cutoff
                ]
            removed_by_date = len(details) - len(filtered_details)
            print(f"date filtering: cutoff_date={cutoff_date}, date_mode={date_mode}, removed: {removed_by_date}")
        else:
            filtered_details = details
            print(f"warning: dataset file {dataset_file} not found, not filtered by date")
    else:
        filtered_details = details
    
    filtered_count = len(filtered_details)
    removed_count = original_count - filtered_count
    
    print(f"original instances count: {original_count}, filtered: {filtered_count}, removed: {removed_count}")
    
    if filtered_count == 0:
        print("warning: no remaining instances after filtering, cannot calculate metrics")
        return
    
    # recalculate metrics
    metrics_accumulator = {
        "codebleu": [],
        "edit_distance": [],
        "jaccard": [],
        "tfidf_cosine": [],
        "file_hit_rate": []
    }
    
    completed_ids = set()
    resolved_ids = set()
    applied_ids = set()
    unresolved_ids = set()
    empty_patch_ids = set()
    
    for detail in filtered_details:
        instance_id = detail.get('instance_id')
        if instance_id:
            completed_ids.add(instance_id)
        
        if detail.get('resolved', False):
            resolved_ids.add(instance_id)
        else:
            unresolved_ids.add(instance_id)
        
        if detail.get('applied', False):
            applied_ids.add(instance_id)
        
        # collect metrics
        if 'CodeBLEU' in detail:
            metrics_accumulator["codebleu"].append(detail['CodeBLEU'])
        if 'normalized_edit_distance' in detail:
            metrics_accumulator["edit_distance"].append(detail['normalized_edit_distance'])
        if 'jaccard_similarity' in detail:
            metrics_accumulator["jaccard"].append(detail['jaccard_similarity'])
        if 'tfidf_cosine_similarity' in detail:
            metrics_accumulator["tfidf_cosine"].append(detail['tfidf_cosine_similarity'])
        if 'file_hit_rate' in detail:
            metrics_accumulator["file_hit_rate"].append(detail['file_hit_rate'])
    
    # calculate filtered metrics
    total = len(completed_ids)
    filtered_metrics = {
        "total_instances": report.get('total_instances', 0) - removed_count,
        "completed_instances": total,
        "resolved_instances": len(resolved_ids),
        "applied_instances": len(applied_ids),
        "unresolved_instances": len(unresolved_ids),
    }
    
    if total > 0:
        filtered_metrics["applied_rate"] = round((len(applied_ids) / total) * 100, 2)
        filtered_metrics["resolved_rate"] = round((len(resolved_ids) / total) * 100, 2)
        
        if metrics_accumulator["codebleu"]:
            filtered_metrics["avg_CodeBLEU"] = round(np.mean(metrics_accumulator["codebleu"]), 4)
        else:
            filtered_metrics["avg_CodeBLEU"] = 0.0
        
        if metrics_accumulator["edit_distance"]:
            filtered_metrics["avg_normalized_edit_distance"] = round(np.mean(metrics_accumulator["edit_distance"]), 4)
        else:
            filtered_metrics["avg_normalized_edit_distance"] = 0.0
        
        if metrics_accumulator["jaccard"]:
            filtered_metrics["avg_jaccard_similarity"] = round(np.mean(metrics_accumulator["jaccard"]), 4)
        else:
            filtered_metrics["avg_jaccard_similarity"] = 0.0
        
        if metrics_accumulator["tfidf_cosine"]:
            filtered_metrics["avg_tfidf_cosine_similarity"] = round(np.mean(metrics_accumulator["tfidf_cosine"]), 4)
        else:
            filtered_metrics["avg_tfidf_cosine_similarity"] = 0.0
        
        if metrics_accumulator["file_hit_rate"]:
            filtered_metrics["file_hit_rate"] = round(np.mean(metrics_accumulator["file_hit_rate"]), 4)
        else:
            filtered_metrics["file_hit_rate"] = 0.0
    else:
        filtered_metrics["applied_rate"] = 0.0
        filtered_metrics["resolved_rate"] = 0.0
        filtered_metrics["avg_CodeBLEU"] = 0.0
        filtered_metrics["avg_normalized_edit_distance"] = 0.0
        filtered_metrics["avg_jaccard_similarity"] = 0.0
        filtered_metrics["avg_tfidf_cosine_similarity"] = 0.0
        filtered_metrics["file_hit_rate"] = 0.0
    
    # parse model name, parameters and run_id from file path
    path_parts = report_path.parts
    model_name = None
    temperature = None
    n = None
    run_id = None
    
    for i, part in enumerate(path_parts):
        if part.startswith("T="):
            if i > 0:
                model_name = path_parts[i - 1]
            try:
                temperature = float(part.split("=")[1])
            except (ValueError, IndexError):
                pass
        elif part.startswith("n="):
            try:
                n = int(part.split("=")[1])
            except (ValueError, IndexError):
                pass
    
    filename = report_path.stem
    if "-output" in filename:
        match = re.search(r'(.+?)-output$', filename)
        if match:
            prefix = match.group(1)
            last_dash_idx = prefix.rfind('-')
            if last_dash_idx != -1:
                run_id = prefix[last_dash_idx + 1:]
            else:
                run_id = prefix
    
    # print filtered summary
    print("\n" + "=" * 80)
    print("analysis report summary (filtered)")
    print("=" * 80)
    
    print("\n„Äêexperiment configuration„Äë")
    if model_name:
        print(f"model: {model_name}")
    if temperature is not None:
        print(f"temperature: {temperature}")
    if n is not None:
        print(f"number of instances: {n}")
    if run_id:
        print(f"run ID: {run_id}")
    print(f"report file: {report_path}")
    print(f"filter file: {filter_ids_file}")
    print(f"removed instances count: {removed_count}")
    if cutoff_date:
        print(f"cutoff date: {cutoff_date} ({date_mode})")
    
    print("\n„Äêapplied and resolved rate statistics„Äë")
    print(f"applied rate: {filtered_metrics['applied_rate']}%")
    print(f"resolved rate: {filtered_metrics['resolved_rate']}%")
    print(f"file hit rate (recall): {filtered_metrics['file_hit_rate']:.4f}")
    
    print("\n„Äêsimilarity metrics average values„Äë")
    print(f"avg CodeBLEU: {filtered_metrics['avg_CodeBLEU']}")
    print(f"avg normalized edit distance: {filtered_metrics['avg_normalized_edit_distance']}")
    print(f"avg Jaccard similarity: {filtered_metrics['avg_jaccard_similarity']}")
    print(f"avg TF-IDF cosine similarity: {filtered_metrics['avg_tfidf_cosine_similarity']}")
    
    print("\n„Äêinstance statistics„Äë")
    print(f"total instances count: {filtered_metrics['total_instances']}")
    print(f"completed instances count: {filtered_metrics['completed_instances']}")
    print(f"resolved instances count: {filtered_metrics['resolved_instances']}")
    print(f"applied instances count: {filtered_metrics['applied_instances']}")
    
    print("\n" + "=" * 80)


def _parse_cutoff_date(cutoff_date: str):
    """parse cutoff date, support 20250101 or 2025-01-01 format, return date object for comparison"""
    s = cutoff_date.strip().replace("-", "").replace("/", "")
    if len(s) == 8:
        return dt(int(s[:4]), int(s[4:6]), int(s[6:8])).date()
    raise ValueError(f"cutoff_date format should be YYYYMMDD or YYYY-MM-DD: {cutoff_date}")


def _parse_created_at(created_at: str):
    """parse created_at string (e.g. 2025-09-19T21:15:54Z), return date or None"""
    if not created_at:
        return None
    try:
        # ISO format 2025-09-19T21:15:54Z
        if "T" in created_at:
            return dt.fromisoformat(created_at.replace("Z", "+00:00")).date()
        # only date 2025-01-01
        if len(created_at) >= 10:
            return dt.strptime(created_at[:10], "%Y-%m-%d").date()
        return None
    except (ValueError, TypeError):
        return None


def print_analysis_summary(
    report_path: Union[Path, str],
    cutoff_date: Optional[str] = None,
    date_mode: str = "before",
):
    """
    print analysis report summary information
    
    Args:
        report_path: analysis report JSON file path
        cutoff_date: cutoff date (optional), format as YYYYMMDD or YYYY-MM-DD, e.g. "20250101"
        date_mode: with cutoff_date. "before" only count instances before the date; "after" only count instances after the date
    """
    report_path = Path(report_path)
    
    # parse model name, parameters and run_id from file path
    # path format: {SRC_EXPERIMENTS}/{model_name}/T={temperature}/n={n}/{name}-{run_id}-output.json
    path_parts = report_path.parts
    model_name = None
    temperature = None
    n = None
    run_id = None
    
    # find model_name (usually in the path)
    for i, part in enumerate(path_parts):
        if part.startswith("T="):
            # the previous part should be model_name
            if i > 0:
                model_name = path_parts[i - 1]
            # extract temperature
            try:
                temperature = float(part.split("=")[1])
            except (ValueError, IndexError):
                pass
        elif part.startswith("n="):
            # extract n
            try:
                n = int(part.split("=")[1])
            except (ValueError, IndexError):
                pass
    
    # extract run_id from file name
    # file name format: {name}-{run_id}-output.json
    # e.g. all-0.2-output.json or all-0.2_body_issue-output.json
    filename = report_path.stem  # remove .json extension
    if "-output" in filename:
        # extract run_id: in the last "-" and "-output"
        match = re.search(r'(.+?)-output$', filename)
        if match:
            prefix = match.group(1)
            last_dash_idx = prefix.rfind('-')
            if last_dash_idx != -1:
                run_id = prefix[last_dash_idx + 1:]
            else:
                run_id = prefix
    
    # filter by date: need to read created_at from dataset, then filter instance_details and recalculate metrics
    if cutoff_date is not None and run_id:
        cutoff = _parse_cutoff_date(cutoff_date)
        dataset_file = SRC_INF_BENCHMARK_DATA / f"all-task-instances_{run_id}.jsonl"
        if not dataset_file.exists():
            print(f"‚ö†Ô∏è cannot find dataset file {dataset_file}, cannot filter by date, will output all results")
            metrics = read_analysis_report(report_path)
        else:
            # build instance_id -> created_at (date)
            instance_date_map = {}
            with open(dataset_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if not line.strip():
                        continue
                    try:
                        data = json.loads(line)
                        iid = data.get("instance_id")
                        ca = data.get("created_at", "")
                        if iid:
                            instance_date_map[iid] = _parse_created_at(ca)
                    except json.JSONDecodeError:
                        continue
            # read full report
            with open(report_path, 'r', encoding='utf-8') as f:
                report = json.load(f)
            details = report.get("instance_details", [])
            # filter by date
            if date_mode == "before":
                filtered_details = [
                    d for d in details
                    if instance_date_map.get(d.get("instance_id")) is not None
                    and instance_date_map[d["instance_id"]] < cutoff
                ]
            else:
                filtered_details = [
                    d for d in details
                    if instance_date_map.get(d.get("instance_id")) is not None
                    and instance_date_map[d["instance_id"]] >= cutoff
                ]
            removed = len(details) - len(filtered_details)
            print(f"date filtering: cutoff_date={cutoff_date}, date_mode={date_mode}")
            print(f"original instances count: {len(details)}, filtered: {len(filtered_details)}, removed: {removed}")
            if not filtered_details:
                print("warning: no instances after filtering, cannot calculate metrics")
                return
            # recalculate metrics
            codebleu_list = []
            edit_dist_list = []
            jaccard_list = []
            tfidf_list = []
            hit_list = []
            resolved_count = applied_count = 0
            for d in filtered_details:
                if d.get("resolved"):
                    resolved_count += 1
                if d.get("applied"):
                    applied_count += 1
                if "CodeBLEU" in d:
                    codebleu_list.append(d["CodeBLEU"])
                if "normalized_edit_distance" in d:
                    edit_dist_list.append(d["normalized_edit_distance"])
                if "jaccard_similarity" in d:
                    jaccard_list.append(d["jaccard_similarity"])
                if "tfidf_cosine_similarity" in d:
                    tfidf_list.append(d["tfidf_cosine_similarity"])
                if "file_hit_rate" in d:
                    hit_list.append(d["file_hit_rate"])
            total = len(filtered_details)
            metrics = {
                "total_instances": total,
                "completed_instances": total,
                "resolved_instances": resolved_count,
                "applied_instances": applied_count,
                "applied_rate": round((applied_count / total) * 100, 2) if total else 0.0,
                "resolved_rate": round((resolved_count / total) * 100, 2) if total else 0.0,
                "file_hit_rate": round(np.mean(hit_list), 4) if hit_list else 0.0,
                "avg_CodeBLEU": round(np.mean(codebleu_list), 4) if codebleu_list else 0.0,
                "avg_normalized_edit_distance": round(np.mean(edit_dist_list), 4) if edit_dist_list else 0.0,
                "avg_jaccard_similarity": round(np.mean(jaccard_list), 4) if jaccard_list else 0.0,
                "avg_tfidf_cosine_similarity": round(np.mean(tfidf_list), 4) if tfidf_list else 0.0,
            }
    else:
        metrics = read_analysis_report(report_path)
    
    print("\n" + "=" * 80)
    title = "analysis report summary"
    if cutoff_date:
        title += f"Ôºàonly {date_mode == 'before' and 'before' or 'after'} {cutoff_date}Ôºâ"
    print(title)
    print("=" * 80)
    
    # print experiment information
    print("\n„Äêexperiment configuration„Äë")
    if model_name:
        print(f"model: {model_name}")
    if temperature is not None:
        print(f"temperature: {temperature}")
    if n is not None:
        print(f"number of instances: {n}")
    if run_id:
        print(f"run ID: {run_id}")
    print(f"report file: {report_path}")
    if cutoff_date:
        print(f"cutoff date: {cutoff_date} ({date_mode})")
    
    print("\n„Äêapplied and resolved rate statistics„Äë")
    print(f"applied rate: {metrics['applied_rate']}%")
    print(f"resolved rate: {metrics['resolved_rate']}%")
    print(f"file hit rate (recall): {metrics['file_hit_rate']:.4f}")
    
    print("\n„Äêsimilarity metrics average values„Äë")
    print(f"avg CodeBLEU: {metrics['avg_CodeBLEU']}")
    print(f"avg normalized edit distance: {metrics['avg_normalized_edit_distance']}")
    print(f"avg Jaccard similarity: {metrics['avg_jaccard_similarity']}")
    print(f"avg TF-IDF cosine similarity: {metrics['avg_tfidf_cosine_similarity']}")
    
    print("\n„Äêinstance statistics„Äë")
    print(f"total instances count: {metrics['total_instances']}")
    print(f"completed instances count: {metrics['completed_instances']}")
    print(f"resolved instances count: {metrics['resolved_instances']}")
    print(f"applied instances count: {metrics['applied_instances']}")
    
    print("\n" + "=" * 80)


def detect_import_edits(patch_content):
    """
    detect if patch contains import dependency edit lines
    :param patch_content: diff patch content (string)
    :return: (has_import_edits: bool, matched_lines: list) - whether import edit exists, matched specific lines
    """
    import_edit_pattern = r'^[+-]\s*(import|from|improt)\b'  # \b ensure it's a complete keyword, avoid matching importxxx

    if not patch_content:
        return False, []
    # match by line, return all matching lines
    matched_lines = re.findall(import_edit_pattern, patch_content, re.MULTILINE)
    # remove duplicates and return result (avoid duplicate lines)
    unique_matched = list(set(matched_lines))
    return len(unique_matched) > 0, [line for line in patch_content.split('\n') if re.match(import_edit_pattern, line)]


if __name__ == "__main__":
    # Example:
    #   python -m editbench.evaluation.run_evaluation run --dataset-name ./infbench/all-task-instances_0.2.jsonl --predictions-path ./experiments/model/T=0/n=1/all-task-instances_0.2.jsonl --run-id 0.2 [--max-workers 4] [--timeout 600]
    #   python -m editbench.evaluation.run_evaluation summary --report-path ./experiments/model/T=0/n=1/all-0.2-output.json [--filter-ids-file ./union-all.json] [--cutoff-date 20250101] [--date-mode before]
    #   python -m editbench.evaluation.run_evaluation batch --dataset-name all --model claude-sonnet-4-5-20250929 deepseek-v3.2 --run-id 0.2 [--sampled-ids-file ./sampled.json]
    try:
        resource.setrlimit(resource.RLIMIT_NOFILE, (1048576, 1048576))
        print("Global file descriptor limit set to: 1048576 (1M)")
    except (ValueError, OSError) as e:
        current_limit = resource.getrlimit(resource.RLIMIT_NOFILE)
        print(f"Warning: Could not set global file descriptor limit. Current limit: {current_limit}")
        print(f"Error: {e}")

    parser = argparse.ArgumentParser(
        description="Run evaluation harness (Docker run + report) and print analysis summary.",
    )
    subparsers = parser.add_subparsers(dest="command", required=True, help="Subcommand")

    # run: evaluation (supports single or multiple datasets/models/run_ids)
    p_run = subparsers.add_parser("run", help="Run evaluation for dataset(s) + predictions + run_id(s)")
    p_run.add_argument("--dataset_name", type=str, nargs="+", default=["all"], 
                       help="Dataset name(s): 'all', 'owner/repo_name', or full path (default: all)")
    p_run.add_argument("--predictions_path", type=str, nargs="+", default=None, 
                       help="Predictions jsonl/json path(s) or 'gold' (optional, default: 'gold' if model not specified)")
    p_run.add_argument("--run_id", type=str, nargs="+", default=["0.2"],
                       choices=["0.2", "0.4", "0.6", "0.8", "0.2_bm25_1", "0.2_bm25_3", "0.2_bm25_5", "0.2_body_issue", "None_body_issue"],
                       help="Run ID(s) (default: 0.2, e.g. 0.2 0.4 0.6 0.8, or variants like 0.2_bm25_1)")
    p_run.add_argument("--model", type=str, nargs="+", default=None, 
                       help="Model name(s), e.g. claude-sonnet-4-5-20250929 (optional, auto-detect from predictions if not specified)")
    p_run.add_argument("--split", type=str, default="", help="Split name (default: empty)")
    p_run.add_argument("--instance_ids", type=str, nargs="*", default=None, 
                       help="Instance IDs to run; if omitted, run all")
    p_run.add_argument("--sampled_ids_file", type=str, default=None, 
                       help="JSON file with sampled_instance_ids (overrides instance_ids if set)")
    p_run.add_argument("--max_workers", type=int, default=4, 
                       help="Max concurrent workers (default: 4)")
    p_run.add_argument("--force_rebuild", action="store_true", 
                       help="Force rebuild Docker images")
    p_run.add_argument("--cache_level", type=str, default="eval", 
                       choices=("none", "base", "env", "eval"), help="Cache level (default: eval)")
    p_run.add_argument("--clean", action="store_true", 
                       help="Clean images above cache level after run")
    p_run.add_argument("--open_file_limit", type=int, default=1048576, 
                       help="RLIMIT_NOFILE (default: 1048576)")
    p_run.add_argument("--timeout", type=int, default=600, 
                       help="Timeout per instance in seconds (default: 600)")
    p_run.add_argument("--max_eval_images", type=int, default=None, 
                       help="Max eval images to keep when cache_level=eval (default: None)")

    # summary: print analysis summary
    p_sum = subparsers.add_parser("summary", help="Print analysis summary for an output.json report")
    p_sum.add_argument("--report_path", type=str, required=True, 
                       help="Path to output.json report")
    p_sum.add_argument("--filter_ids_file", type=str, default=None, 
                       help="JSON file with union_filter_instance_ids to exclude (optional)")
    p_sum.add_argument("--cutoff_date", type=str, default=None, 
                       help="Cutoff date YYYYMMDD or YYYY-MM-DD (optional)")
    p_sum.add_argument("--date_mode", type=str, default="before", 
                       choices=("before", "after"), help="With cutoff_date: before | after (default: before)")

    args = parser.parse_args()

    if args.command == "run":
        # Get arguments (argparse converts hyphens to underscores)
        dataset_names = args.dataset_name
        predictions_paths = args.predictions_path
        run_ids = args.run_id
        models = args.model if args.model else []
        sampled_ids_file = args.sampled_ids_file
        instance_ids = args.instance_ids
        max_workers = args.max_workers
        force_rebuild = args.force_rebuild
        cache_level = args.cache_level
        clean = args.clean
        open_file_limit = args.open_file_limit
        timeout = args.timeout
        max_eval_images = args.max_eval_images
        
        # Auto-infer predictions_path if not provided
        if predictions_paths is None:
            if models:
                # If model is specified, predictions_path will be auto-constructed
                predictions_paths = []
            else:
                # If no model and no predictions_path, default to "gold"
                predictions_paths = ["gold"]
        
        # Load sampled instance IDs if provided
        sampled_instance_ids = None
        if sampled_ids_file and Path(sampled_ids_file).exists():
            try:
                with open(sampled_ids_file, "r", encoding="utf-8") as f:
                    sampled_instance_ids = json.load(f).get("sampled_instance_ids", [])
                print(f"Loaded sampled instance IDs from {sampled_ids_file}: {len(sampled_instance_ids)}")
            except Exception as e:
                print(f"Warning: failed to load sampled IDs: {e}")
        
        # If instance_ids is provided, use it; otherwise use sampled_instance_ids
        if instance_ids is None:
            instance_ids = sampled_instance_ids if sampled_instance_ids is not None else []
        
        # Normalize to lists
        if isinstance(dataset_names, str):
            dataset_names = [dataset_names]
        if isinstance(predictions_paths, str):
            predictions_paths = [predictions_paths]
        if isinstance(run_ids, str):
            run_ids = [run_ids]
        if isinstance(models, str):
            models = [models]
        
        # Unified processing: iterate over all combinations
        # Auto-detect and normalize dataset_name and predictions_path for each combination
        for run_id in run_ids:
            for dataset_name in dataset_names:
                # Determine predictions_path based on whether models are specified
                if models:
                    # If model is specified, iterate over models and construct predictions_path
                    for model_name in models:
                        model_config = MAP_MODEL_TO_COFIG.get(model_name, {"temperature": 0, "n": 1})
                        # Extract name from dataset_name
                        _, name = normalize_dataset_name(dataset_name, run_id)
                        current_predictions_path = f"{SRC_EXPERIMENTS}/{model_name}/T={model_config['temperature']}/n={model_config['n']}/{name}-task-instances_{run_id}.jsonl"
                        
                        print("\n" + "=" * 80)
                        print("Starting task:")
                        print(f"  dataset: {dataset_name}, model: {model_name}, run_id: {run_id}")
                        print(f"  predictions: {current_predictions_path}")
                        print("=" * 80 + "\n")
                        main(
                            dataset_name=dataset_name,
                            split=args.split,
                            instance_ids=instance_ids,
                            predictions_path=current_predictions_path,
                            max_workers=max_workers,
                            force_rebuild=force_rebuild,
                            cache_level=cache_level,
                            clean=clean,
                            open_file_limit=open_file_limit,
                            run_id=run_id,
                            timeout=timeout,
                            max_eval_images=max_eval_images,
                        )
                else:
                    # If model is not specified, use predictions_path directly (can be "gold" or path)
                    for current_predictions_path in predictions_paths:
                        print("\n" + "=" * 80)
                        print("Starting task:")
                        print(f"  dataset: {dataset_name}, run_id: {run_id}")
                        print(f"  predictions: {current_predictions_path}")
                        print("=" * 80 + "\n")
                        main(
                            dataset_name=dataset_name,
                            split=args.split,
                            instance_ids=instance_ids,
                            predictions_path=current_predictions_path,
                            max_workers=max_workers,
                            force_rebuild=force_rebuild,
                            cache_level=cache_level,
                            clean=clean,
                            open_file_limit=open_file_limit,
                            run_id=run_id,
                            timeout=timeout,
                            max_eval_images=max_eval_images,
                        )

    elif args.command == "summary":
        report_path = Path(args.report_path)
        filter_ids_file = args.filter_ids_file
        cutoff_date = args.cutoff_date
        date_mode = args.date_mode
        
        filter_path = Path(filter_ids_file) if filter_ids_file else None
        if filter_path and filter_path.exists():
            print_analysis_summary_filtered(
                report_path,
                filter_path,
                cutoff_date=cutoff_date,
                date_mode=date_mode,
            )
        else:
            if filter_ids_file and not (filter_path and filter_path.exists()):
                print(f"Warning: filter IDs file not found: {filter_ids_file}, printing unfiltered summary.")
            print_analysis_summary(
                report_path,
                cutoff_date=cutoff_date,
                date_mode=date_mode,
            )
