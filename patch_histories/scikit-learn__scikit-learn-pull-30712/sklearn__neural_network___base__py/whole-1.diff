diff --git a/sklearn/neural_network/_base.py b/sklearn/neural_network/_base.py
--- a/sklearn/neural_network/_base.py
+++ b/sklearn/neural_network/_base.py
@@ -20,6 +20,17 @@ def inplace_identity(X):
     # Nothing to do
 
 
+def inplace_exp(X):
+    """Compute the exponential inplace.
+
+    Parameters
+    ----------
+    X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        The input data.
+    """
+    np.exp(X, out=X)
+
+
 def inplace_logistic(X):
     """Compute the logistic function inplace.
 
@@ -68,6 +79,7 @@ def inplace_softmax(X):
 
 ACTIVATIONS = {
     "identity": inplace_identity,
+    "exp": inplace_exp,
     "tanh": inplace_tanh,
     "logistic": inplace_logistic,
     "relu": inplace_relu,
@@ -177,6 +189,33 @@ def squared_loss(y_true, y_pred, sample_weight=None):
     )
 
 
+def poisson_loss(y_true, y_pred, sample_weight=None):
+    """Compute (half of the) Poisson deviance loss for regression.
+
+    Parameters
+    ----------
+    y_true : array-like or label indicator matrix
+        Ground truth (correct) labels.
+
+    y_pred : array-like or label indicator matrix
+        Predicted values, as returned by a regression estimator.
+
+    sample_weight : array-like of shape (n_samples,), default=None
+        Sample weights.
+
+    Returns
+    -------
+    loss : float
+        The degree to which the samples are correctly predicted.
+    """
+    # TODO: Decide what to do with the term `xlogy(y_true, y_true) - y_true`. For now,
+    # it is included. But the _loss module doesn't use it (for performance reasons) and
+    # only adds it as return of constant_to_optimal_zero (mainly for testing).
+    return np.average(
+        xlogy(y_true, y_true / y_pred) - y_true + y_pred, weights=sample_weight, axis=0
+    ).sum()
+
+
 def log_loss(y_true, y_prob, sample_weight=None):
     """Compute Logistic loss for classification.
 
@@ -242,6 +281,7 @@ def binary_log_loss(y_true, y_prob, sample_weight=None):
 
 LOSS_FUNCTIONS = {
     "squared_error": squared_loss,
+    "poisson": poisson_loss,
     "log_loss": log_loss,
     "binary_log_loss": binary_log_loss,
 }
