diff --git a/sklearn/neural_network/_base.py b/sklearn/neural_network/_base.py
--- a/sklearn/neural_network/_base.py
+++ b/sklearn/neural_network/_base.py
@@ -20,6 +20,17 @@ def inplace_identity(X):
     # Nothing to do
 
 
+def inplace_exp(X):
+    """Compute the exponential inplace.
+
+    Parameters
+    ----------
+    X : {array-like, sparse matrix}, shape (n_samples, n_features)
+        The input data.
+    """
+    np.exp(X, out=X)
+
+
 def inplace_logistic(X):
     """Compute the logistic function inplace.
 
@@ -68,6 +79,7 @@ def inplace_softmax(X):
 
 ACTIVATIONS = {
     "identity": inplace_identity,
+    "exp": inplace_exp,
     "tanh": inplace_tanh,
     "logistic": inplace_logistic,
     "relu": inplace_relu,
@@ -177,6 +189,33 @@ def squared_loss(y_true, y_pred, sample_weight=None):
     )
 
 
+def poisson_loss(y_true, y_pred, sample_weight=None):
+    """Compute (half of the) Poisson deviance loss for regression.
+
+    Parameters
+    ----------
+    y_true : array-like or label indicator matrix
+        Ground truth (correct) labels.
+
+    y_pred : array-like or label indicator matrix
+        Predicted values, as returned by a regression estimator.
+
+    sample_weight : array-like of shape (n_samples,), default=None
+        Sample weights.
+
+    Returns
+    -------
+    loss : float
+        The degree to which the samples are correctly predicted.
+    """
+    # TODO: Decide what to do with the term `xlogy(y_true, y_true) - y_true`. For now,
+    # it is included. But the _loss module doesn't use it (for performance reasons) and
+    # only adds it as return of constant_to_optimal_zero (mainly for testing).
+    return np.average(
+        xlogy(y_true, y_true / y_pred) - y_true + y_pred, weights=sample_weight, axis=0
+    ).sum()
+
+
 def log_loss(y_true, y_prob, sample_weight=None):
     """Compute Logistic loss for classification.
 
@@ -242,6 +281,7 @@ def binary_log_loss(y_true, y_prob, sample_weight=None):
 
 LOSS_FUNCTIONS = {
     "squared_error": squared_loss,
+    "poisson": poisson_loss,
     "log_loss": log_loss,
     "binary_log_loss": binary_log_loss,
 }
diff --git a/sklearn/neural_network/_multilayer_perceptron.py b/sklearn/neural_network/_multilayer_perceptron.py
--- a/sklearn/neural_network/_multilayer_perceptron.py
+++ b/sklearn/neural_network/_multilayer_perceptron.py
@@ -399,7 +399,11 @@ def _initialize(self, y, layer_units, dtype):
 
         # Output for regression
         if not is_classifier(self):
-            self.out_activation_ = "identity"
+            if self.loss == "poisson":
+                self.out_activation_ = "exp"
+            else:
+                # loss = "squared_error"
+                self.out_activation_ = "identity"
         # Output for multi class
         elif self._label_binarizer.y_type_ == "multiclass":
             self.out_activation_ = "softmax"
@@ -1378,6 +1382,17 @@ class MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):
 
     Parameters
     ----------
+    loss : {'squared_error', 'poisson'}, default='squared_error'
+        The loss function to use when training the weights. Note that the
+        "squared error" and "poisson" losses actually implement
+        "half squares error" and "half poisson deviance" to simplify the
+        computation of the gradient. Furthermore, the "poisson" loss internally uses
+        a log-link (exponential as the output activation function) and requires
+        ``y >= 0``.
+
+        .. versionchanged:: 1.7
+           Added parameter `loss` and option 'poisson'.
+
     hidden_layer_sizes : array-like of shape(n_layers - 2,), default=(100,)
         The ith element represents the number of neurons in the ith
         hidden layer.
@@ -1646,8 +1661,14 @@ class MLPRegressor(RegressorMixin, BaseMultilayerPerceptron):
     0.98...
     """
 
+    _parameter_constraints: dict = {
+        **BaseMultilayerPerceptron._parameter_constraints,
+        "loss": [StrOptions({"squared_error", "poisson"})],
+    }
+
     def __init__(
         self,
+        loss="squared_error",
         hidden_layer_sizes=(100,),
         activation="relu",
         *,
@@ -1683,7 +1704,7 @@ def __init__(
             learning_rate_init=learning_rate_init,
             power_t=power_t,
             max_iter=max_iter,
-            loss="squared_error",
+            loss=loss,
             shuffle=shuffle,
             random_state=random_state,
             tol=tol,