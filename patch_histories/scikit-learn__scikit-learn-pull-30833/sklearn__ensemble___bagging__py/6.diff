diff --git a/sklearn/ensemble/_bagging.py b/sklearn/ensemble/_bagging.py
--- a/sklearn/ensemble/_bagging.py
+++ b/sklearn/ensemble/_bagging.py
@@ -1038,7 +1038,7 @@
 
         return proba
 
-    def predict_log_proba(self, X):
+    def predict_log_proba(self, X, **params):
         """Predict class log-probabilities for X.
 
         The predicted class log-probabilities of an input sample is computed as
@@ -1050,6 +1050,19 @@
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             The training input samples. Sparse matrices are accepted only if
             they are supported by the base estimator.
+
+        **params : dict
+            Parameters routed to the `predict_log_proba`, the `predict_proba` or the
+            `proba` method of the sub-estimators via the metadata routing API. The
+            routing is tried in the mentioned order depending on whether this method is
+            available on the sub-estimator.
+
+            .. versionadded:: 1.7
+
+                Only available if
+                `sklearn.set_config(enable_metadata_routing=True)` is set. See
+                :ref:`Metadata Routing User Guide <metadata_routing>` for more
+                details.
 
         Returns
         -------
@@ -1057,7 +1070,10 @@
             The class log-probabilities of the input samples. The order of the
             classes corresponds to that in the attribute :term:`classes_`.
         """
+        _raise_for_params(params, self, "predict_log_proba")
+
         check_is_fitted(self)
+
         if hasattr(self.estimator_, "predict_log_proba"):
             # Check data
             X = validate_data(
@@ -1069,6 +1085,12 @@
                 reset=False,
             )
 
+            if _routing_enabled():
+                routed_params = process_routing(self, "predict_log_proba", **params)
+            else:
+                routed_params = Bunch()
+                routed_params.estimator = Bunch(predict_log_proba=Bunch())
+
             # Parallel loop
             n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)
 
@@ -1078,6 +1100,7 @@
                     self.estimators_features_[starts[i] : starts[i + 1]],
                     X,
                     self.n_classes_,
+                    params=routed_params.estimator.predict_log_proba,
                 )
                 for i in range(n_jobs)
             )
@@ -1091,14 +1114,14 @@
             log_proba -= np.log(self.n_estimators)
 
         else:
-            log_proba = np.log(self.predict_proba(X))
+            log_proba = np.log(self.predict_proba(X, **params))
 
         return log_proba
 
     @available_if(
         _estimator_has("decision_function", delegates=("estimators_", "estimator"))
     )
-    def decision_function(self, X):
+    def decision_function(self, X, **params):
         """Average of the decision functions of the base classifiers.
 
         Parameters
@@ -1106,6 +1129,17 @@
         X : {array-like, sparse matrix} of shape (n_samples, n_features)
             The training input samples. Sparse matrices are accepted only if
             they are supported by the base estimator.
+
+        **params : dict
+            Parameters routed to the `decision_function` method of the sub-estimators
+            via the metadata routing API.
+
+            .. versionadded:: 1.7
+
+                Only available if
+                `sklearn.set_config(enable_metadata_routing=True)` is set. See
+                :ref:`Metadata Routing User Guide <metadata_routing>` for more
+                details.
 
         Returns
         -------
@@ -1115,6 +1149,8 @@
             ``classes_``. Regression and binary classification are special
             cases with ``k == 1``, otherwise ``k==n_classes``.
         """
+        _raise_for_params(params, self, "decision_function")
+
         check_is_fitted(self)
 
         # Check data
@@ -1127,6 +1163,12 @@
             reset=False,
         )
 
+        if _routing_enabled():
+            routed_params = process_routing(self, "decision_function", **params)
+        else:
+            routed_params = Bunch()
+            routed_params.estimator = Bunch(decision_function=Bunch())
+
         # Parallel loop
         n_jobs, _, starts = _partition_estimators(self.n_estimators, self.n_jobs)
 
@@ -1135,6 +1177,7 @@
                 self.estimators_[starts[i] : starts[i + 1]],
                 self.estimators_features_[starts[i] : starts[i + 1]],
                 X,
+                params=routed_params.estimator.decision_function,
             )
             for i in range(n_jobs)
         )
