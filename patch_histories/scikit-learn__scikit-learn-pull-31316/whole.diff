diff --git a/sklearn/linear_model/_glm/_newton_solver.py b/sklearn/linear_model/_glm/_newton_solver.py
--- a/sklearn/linear_model/_glm/_newton_solver.py
+++ b/sklearn/linear_model/_glm/_newton_solver.py
@@ -178,21 +178,22 @@ def fallback_lbfgs_solve(self, X, y, sample_weight):
             - self.coef
             - self.converged
         """
+        max_iter = self.max_iter - self.iteration
         opt_res = scipy.optimize.minimize(
             self.linear_loss.loss_gradient,
             self.coef,
             method="L-BFGS-B",
             jac=True,
             options={
-                "maxiter": self.max_iter - self.iteration,
+                "maxiter": max_iter,
                 "maxls": 50,  # default is 20
                 "iprint": self.verbose - 1,
                 "gtol": self.tol,
                 "ftol": 64 * np.finfo(np.float64).eps,
             },
             args=(X, y, sample_weight, self.l2_reg_strength, self.n_threads),
         )
-        self.iteration += _check_optimize_result("lbfgs", opt_res)
+        self.iteration += _check_optimize_result("lbfgs", opt_res, max_iter=max_iter)
         self.coef = opt_res.x
         self.converged = opt_res.status == 0
 
diff --git a/sklearn/linear_model/_glm/glm.py b/sklearn/linear_model/_glm/glm.py
--- a/sklearn/linear_model/_glm/glm.py
+++ b/sklearn/linear_model/_glm/glm.py
@@ -282,7 +282,9 @@ def fit(self, X, y, sample_weight=None):
                 },
                 args=(X, y, sample_weight, l2_reg_strength, n_threads),
             )
-            self.n_iter_ = _check_optimize_result("lbfgs", opt_res)
+            self.n_iter_ = _check_optimize_result(
+                "lbfgs", opt_res, max_iter=self.max_iter
+            )
             coef = opt_res.x
         elif self.solver == "newton-cholesky":
             sol = NewtonCholeskySolver(
diff --git a/sklearn/utils/optimize.py b/sklearn/utils/optimize.py
--- a/sklearn/utils/optimize.py
+++ b/sklearn/utils/optimize.py
@@ -352,25 +352,37 @@ def _check_optimize_result(solver, result, max_iter=None, extra_warning_msg=None
     """
     # handle both scipy and scikit-learn solver names
     if solver == "lbfgs":
-        if result.status != 0:
-            result_message = result.message
+        if max_iter is not None:
+            # In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.
+            # See https://github.com/scipy/scipy/issues/7854
+            n_iter_i = min(result.nit, max_iter)
+        else:
+            n_iter_i = result.nit
 
+        if result.status != 0:
             warning_msg = (
-                "{} failed to converge (status={}):\n{}.\n\n"
-                "Increase the number of iterations (max_iter) "
-                "or scale the data as shown in:\n"
+                f"{solver} failed to converge after {n_iter_i} iteration(s) "
+                f"(status={result.status}):\n"
+                f"{result.message}\n"
+            )
+            # Append a recommendation to increase iterations only when the
+            # number of iterations reaches the maximum allowed (max_iter),
+            # as this suggests the optimization may have been prematurely
+            # terminated due to the iteration limit.
+            if max_iter is not None and n_iter_i == max_iter:
+                warning_msg += (
+                    f"\nIncrease the number of iterations to improve the "
+                    f"convergence (max_iter={max_iter})."
+                )
+            warning_msg += (
+                "\nYou might also want to scale the data as shown in:\n"
                 "    https://scikit-learn.org/stable/modules/"
                 "preprocessing.html"
-            ).format(solver, result.status, result_message)
+            )
             if extra_warning_msg is not None:
                 warning_msg += "\n" + extra_warning_msg
             warnings.warn(warning_msg, ConvergenceWarning, stacklevel=2)
-        if max_iter is not None:
-            # In scipy <= 1.0.0, nit may exceed maxiter for lbfgs.
-            # See https://github.com/scipy/scipy/issues/7854
-            n_iter_i = min(result.nit, max_iter)
-        else:
-            n_iter_i = result.nit
+
     else:
         raise NotImplementedError
 