diff --git a/sphinx/builders/__init__.py b/sphinx/builders/__init__.py
--- a/sphinx/builders/__init__.py
+++ b/sphinx/builders/__init__.py
@@ -14,6 +14,7 @@
 from docutils.utils import DependencyList
 
 from sphinx._cli.util.colour import bold
+from sphinx.deprecation import _deprecation_warning
 from sphinx.environment import (
     CONFIG_CHANGED_REASON,
     CONFIG_OK,
@@ -114,7 +115,7 @@ def __init__(self, app: Sphinx, env: BuildEnvironment) -> None:
         self.doctreedir = app.doctreedir
         ensuredir(self.doctreedir)
 
-        self.app: Sphinx = app
+        self._app: Sphinx = app
         self.env: BuildEnvironment = env
         self.env.set_versioning_method(self.versioning_method, self.versioning_compare)
         self.events: EventManager = app.events
@@ -136,9 +137,15 @@ def __init__(self, app: Sphinx, env: BuildEnvironment) -> None:
         self.parallel_ok = False
         self.finish_tasks: Any = None
 
+    @property
+    def app(self) -> Sphinx:
+        cls_name = self.__class__.__qualname__
+        _deprecation_warning(__name__, f'{cls_name}.app', remove=(10, 0))
+        return self._app
+
     @property
     def _translator(self) -> NullTranslations | None:
-        return self.app.translator
+        return self._app.translator
 
     def get_translator_class(self, *args: Any) -> type[nodes.NodeVisitor]:
         """Return a class of translator."""
@@ -258,7 +265,7 @@ def cat2relpath(cat: CatalogInfo, srcdir: Path = self.srcdir) -> str:
             __('writing output... '),
             'darkgreen',
             len(catalogs),
-            self.app.verbosity,
+            self._app.verbosity,
             stringify_func=cat2relpath,
         ):
             catalog.write_mo(
@@ -397,14 +404,14 @@ def build(
         # while reading, collect all warnings from docutils
         with (
             nullcontext()
-            if self.app._exception_on_warning
+            if self._app._exception_on_warning
             else logging.pending_warnings()
         ):
             updated_docnames = set(self.read())
 
         doccount = len(updated_docnames)
         logger.info(bold(__('looking for now-outdated files... ')), nonl=True)
-        updated_docnames.update(self.env.check_dependents(self.app, updated_docnames))
+        updated_docnames.update(self.env.check_dependents(self._app, updated_docnames))
         outdated = len(updated_docnames) - doccount
         if outdated:
             logger.info(__('%d found'), outdated)
@@ -422,14 +429,14 @@ def build(
                 pickle.dump(self.env, f, pickle.HIGHEST_PROTOCOL)
 
             # global actions
-            self.app.phase = BuildPhase.CONSISTENCY_CHECK
+            self._app.phase = BuildPhase.CONSISTENCY_CHECK
             with progress_message(__('checking consistency')):
                 self.env.check_consistency()
         else:
             if method == 'update' and not docnames:
                 logger.info(bold(__('no targets are out of date.')))
 
-        self.app.phase = BuildPhase.RESOLVING
+        self._app.phase = BuildPhase.RESOLVING
 
         # filter "docnames" (list of outdated files) by the updated
         # found_docs of the environment; this will remove docs that
@@ -438,14 +445,14 @@ def build(
             docnames = set(docnames) & self.env.found_docs
 
         # determine if we can write in parallel
-        if parallel_available and self.app.parallel > 1 and self.allow_parallel:
-            self.parallel_ok = self.app.is_parallel_allowed('write')
+        if parallel_available and self._app.parallel > 1 and self.allow_parallel:
+            self.parallel_ok = self._app.is_parallel_allowed('write')
         else:
             self.parallel_ok = False
 
         #  create a task executor to use for misc. "finish-up" tasks
         # if self.parallel_ok:
-        #     self.finish_tasks = ParallelTasks(self.app.parallel)
+        #     self.finish_tasks = ParallelTasks(self._app.parallel)
         # else:
         # for now, just execute them serially
         self.finish_tasks = SerialTasks()
@@ -508,13 +515,13 @@ def read(self) -> list[str]:
         self.events.emit('env-before-read-docs', self.env, docnames)
 
         # check if we should do parallel or serial read
-        if parallel_available and self.app.parallel > 1:
-            par_ok = self.app.is_parallel_allowed('read')
+        if parallel_available and self._app.parallel > 1:
+            par_ok = self._app.is_parallel_allowed('read')
         else:
             par_ok = False
 
         if par_ok:
-            self._read_parallel(docnames, nproc=self.app.parallel)
+            self._read_parallel(docnames, nproc=self._app.parallel)
         else:
             self._read_serial(docnames)
 
@@ -576,7 +583,7 @@ def _read_serial(self, docnames: list[str]) -> None:
             __('reading sources... '),
             'purple',
             len(docnames),
-            self.app.verbosity,
+            self._app.verbosity,
         ):
             # remove all inventory entries for that file
             self.events.emit('env-purge-doc', self.env, docname)
@@ -589,7 +596,11 @@ def _read_parallel(self, docnames: list[str], nproc: int) -> None:
         # create a status_iterator to step progressbar after reading a document
         # (see: ``merge()`` function)
         progress = status_iterator(
-            chunks, __('reading sources... '), 'purple', len(chunks), self.app.verbosity
+            chunks,
+            __('reading sources... '),
+            'purple',
+            len(chunks),
+            self._app.verbosity,
         )
 
         # clear all outdated docs at once
@@ -598,15 +609,15 @@ def _read_parallel(self, docnames: list[str], nproc: int) -> None:
             self.env.clear_doc(docname)
 
         def read_process(docs: list[str]) -> bytes:
-            self.env.app = self.app
+            self.env._app = self._app
             for docname in docs:
                 self.read_doc(docname, _cache=False)
             # allow pickling self to send it back
             return pickle.dumps(self.env, pickle.HIGHEST_PROTOCOL)
 
         def merge(docs: list[str], otherenv: bytes) -> None:
             env = pickle.loads(otherenv)
-            self.env.merge_info_from(docs, env, self.app)
+            self.env.merge_info_from(docs, env, self._app)
 
             next(progress)
 
@@ -630,8 +641,8 @@ def read_doc(self, docname: str, *, _cache: bool = True) -> None:
             env.note_dependency(docutils_conf)
 
         filename = str(env.doc2path(docname))
-        filetype = get_filetype(self.app.config.source_suffix, filename)
-        publisher = self.env._registry.get_publisher(self.app, filetype)
+        filetype = get_filetype(self._app.config.source_suffix, filename)
+        publisher = self.env._registry.get_publisher(self._app, filetype)
         self.env.current_document._parser = publisher.parser
         # record_dependencies is mutable even though it is in settings,
         # explicitly re-initialise for each document
@@ -744,34 +755,34 @@ def write_documents(self, docnames: Set[str]) -> None:
         if self.parallel_ok:
             # number of subprocesses is parallel-1 because the main process
             # is busy loading doctrees and doing write_doc_serialized()
-            self._write_parallel(sorted_docnames, nproc=self.app.parallel - 1)
+            self._write_parallel(sorted_docnames, nproc=self._app.parallel - 1)
         else:
             self._write_serial(sorted_docnames)
 
     def _write_serial(self, docnames: Sequence[str]) -> None:
         with (
             nullcontext()
-            if self.app._exception_on_warning
+            if self._app._exception_on_warning
             else logging.pending_warnings()
         ):
             for docname in status_iterator(
                 docnames,
                 __('writing output... '),
                 'darkgreen',
                 len(docnames),
-                self.app.verbosity,
+                self._app.verbosity,
             ):
-                _write_docname(docname, app=self.app, env=self.env, builder=self)
+                _write_docname(docname, app=self._app, env=self.env, builder=self)
 
     def _write_parallel(self, docnames: Sequence[str], nproc: int) -> None:
         def write_process(docs: list[tuple[str, nodes.document]]) -> None:
-            self.app.phase = BuildPhase.WRITING
+            self._app.phase = BuildPhase.WRITING
             for docname, doctree in docs:
                 self.write_doc(docname, doctree)
 
         # warm up caches/compile templates using the first document
         firstname, docnames = docnames[0], docnames[1:]
-        _write_docname(firstname, app=self.app, env=self.env, builder=self)
+        _write_docname(firstname, app=self._app, env=self.env, builder=self)
 
         tasks = ParallelTasks(nproc)
         chunks = make_chunks(docnames, nproc)
@@ -783,13 +794,13 @@ def write_process(docs: list[tuple[str, nodes.document]]) -> None:
             __('writing output... '),
             'darkgreen',
             len(chunks),
-            self.app.verbosity,
+            self._app.verbosity,
         )
 
         def on_chunk_done(args: list[tuple[str, nodes.document]], result: None) -> None:
             next(progress)
 
-        self.app.phase = BuildPhase.RESOLVING
+        self._app.phase = BuildPhase.RESOLVING
         for chunk in chunks:
             arg = []
             for docname in chunk:
